{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ex03_moritz_eck.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"GyFe-In3YJaI","colab_type":"text"},"cell_type":"markdown","source":["# Deep Learning for Natural Language Processing: Exercise 03\n","Moritz Eck (14-715-296)<br/>\n","University of Zurich\n","\n","Please see the section right at the bottom of this notebook for the discussion of the results as well as the answers to the exercise questions."]},{"metadata":{"id":"wDnQYGD-YN_N","colab_type":"text"},"cell_type":"markdown","source":["### Mount Google Drive (Please do this step first => only needs to be done once!)\n","\n","This mounts the user's Google Drive directly.\n","\n","On my personal machine inside the Google Drive folder the input files are stored in the following folder:<br/> \n","**~/Google Drive/Colab Notebooks/ex02/**\n","\n","Below I have defined the default filepath as **default_fp = 'drive/Colab Notebooks/ex02/'**.<br/>\n","Please change the filepath to the location where you have the input file and the glove embeddings saved."]},{"metadata":{"id":"i40DOPHRYL2U","colab_type":"code","colab":{}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","\n","import getpass\n","\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mKjkJyV2YWMp","colab_type":"text"},"cell_type":"markdown","source":["**Mount Google Drive**"]},{"metadata":{"id":"AJ8JSYOUYTis","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X0W5PvM1YcXF","colab_type":"text"},"cell_type":"markdown","source":["### Install the required packages"]},{"metadata":{"id":"p-dsu_7QYelR","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install pandas\n","!pip install numpy\n","!pip install tensorflow"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3BJeBrdWYmql","colab_type":"text"},"cell_type":"markdown","source":["### Check that the GPU is used"]},{"metadata":{"id":"TFcDcZ_QYmVg","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","\n","print('Found GPU at: {}'.format(device_name))\n","\n","from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T4UtZ2YTOohS","colab_type":"text"},"cell_type":"markdown","source":["### Helper Functions"]},{"metadata":{"id":"hpkQJm9PO24q","colab_type":"code","colab":{}},"cell_type":"code","source":["# encoding: UTF-8\n","# Copyright 2017 Google.com\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","import numpy as np\n","import glob\n","import sys\n","\n","# size of the alphabet that we work with\n","ALPHASIZE = 98\n","\n","# Specification of the supported alphabet (subset of ASCII-7)\n","# 10 line feed LF\n","# 32-64 numbers and punctuation\n","# 65-90 upper-case letters\n","# 91-97 more punctuation\n","# 97-122 lower-case letters\n","# 123-126 more punctuation\n","def convert_from_alphabet(a):\n","    \"\"\"Encode a character\n","    :param a: one character\n","    :return: the encoded value\n","    \"\"\"\n","    if a == 9:\n","        return 1\n","    if a == 10:\n","        return 127 - 30  # LF\n","    elif 32 <= a <= 126:\n","        return a - 30\n","    else:\n","        return 0  # unknown\n","\n","\n","# encoded values:\n","# unknown = 0\n","# tab = 1\n","# space = 2\n","# all chars from 32 to 126 = c-30\n","# LF mapped to 127-30\n","def convert_to_alphabet(c, avoid_tab_and_lf=False):\n","    \"\"\"Decode a code point\n","    :param c: code point\n","    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n","    :return: decoded character\n","    \"\"\"\n","    if c == 1:\n","        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n","    if c == 127 - 30:\n","        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n","    if 32 <= c + 30 <= 126:\n","        return c + 30\n","    else:\n","        return 0  # unknown\n","\n","\n","def encode_text(s):\n","    \"\"\"Encode a string.\n","    :param s: a text string\n","    :return: encoded list of code points\n","    \"\"\"\n","    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n","\n","\n","def decode_to_text(c, avoid_tab_and_lf=False):\n","    \"\"\"Decode an encoded string.\n","    :param c: encoded list of code points\n","    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n","    :return:\n","    \"\"\"\n","    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n","\n","\n","def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n","    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n","    according to the provided probabilities. If topn is specified, only the\n","    topn highest probabilities are taken into account.\n","    :param probabilities: a list of size ALPHASIZE with individual probabilities\n","    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n","    :return: a random integer\n","    \"\"\"\n","    p = np.squeeze(probabilities)\n","    p[np.argsort(p)[:-topn]] = 0\n","    p = p / np.sum(p)\n","    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n","\n","\n","def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n","    \"\"\"\n","    Divides the data into batches of sequences so that all the sequences in one batch\n","    continue in the next batch. This is a generator that will keep returning batches\n","    until the input data has been seen nb_epochs times. Sequences are continued even\n","    between epochs, apart from one, the one corresponding to the end of raw_data.\n","    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n","    :param raw_data: the training text\n","    :param batch_size: the size of a training minibatch\n","    :param sequence_size: the unroll size of the RNN\n","    :param nb_epochs: number of epochs to train on\n","    :return:\n","        x: one batch of training sequences\n","        y: on batch of target sequences, i.e. training sequences shifted by 1\n","        epoch: the current epoch number (starting at 0)\n","    \"\"\"\n","    data = np.array(raw_data)\n","    data_len = data.shape[0]\n","    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n","    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n","    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n","    rounded_data_len = nb_batches * batch_size * sequence_size\n","    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n","    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n","\n","    for epoch in range(nb_epochs):\n","        for batch in range(nb_batches):\n","            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n","            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n","            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n","            y = np.roll(y, -epoch, axis=0)\n","            yield x, y, epoch\n","\n","\n","def find_book(index, bookranges):\n","    return next(\n","        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n","\n","\n","def find_book_index(index, bookranges):\n","    return next(\n","        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n","\n","\n","def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n","    \"\"\"Display utility for printing learning statistics\"\"\"\n","    print()\n","    # epoch_size in number of batches\n","    batch_size = X.shape[0]  # batch_size in number of sequences\n","    sequence_len = X.shape[1]  # sequence_len in number of characters\n","    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n","    for k in range(1):\n","        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n","        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n","        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n","        bookname = find_book(index_in_epoch, bookranges)\n","        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n","        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n","        loss_string = \"loss: {:.5f}\".format(losses[k])\n","        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n","        print(print_string.format(decx, decy, loss_string))\n","        index += sequence_len\n","    # box formatting characters:\n","    # │ \\u2502\n","    # ─ \\u2500\n","    # └ \\u2514\n","    # ┘ \\u2518\n","    # ┴ \\u2534\n","    # ┌ \\u250C\n","    # ┐ \\u2510\n","    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n","    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n","    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n","    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n","    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n","    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n","    print(footer)\n","    # print statistics\n","    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n","    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n","    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n","    print()\n","    print(\"TRAINING STATS: {}\".format(stats))\n","\n","\n","class Progress:\n","    \"\"\"Text mode progress bar.\n","    Usage:\n","            p = Progress(30)\n","            p.step()\n","            p.step()\n","            p.step(start=True) # to restart form 0%\n","    The progress bar displays a new header at each restart.\"\"\"\n","    def __init__(self, maxi, size=100, msg=\"\"):\n","        \"\"\"\n","        :param maxi: the number of steps required to reach 100%\n","        :param size: the number of characters taken on the screen by the progress bar\n","        :param msg: the message displayed in the header of the progress bat\n","        \"\"\"\n","        self.maxi = maxi\n","        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n","        self.header_printed = False\n","        self.msg = msg\n","        self.size = size\n","\n","    def step(self, reset=False):\n","        if reset:\n","            self.__init__(self.maxi, self.size, self.msg)\n","        if not self.header_printed:\n","            self.__print_header()\n","        next(self.p)\n","\n","    def __print_header(self):\n","        print()\n","        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n","        print(format_string.format(self.msg))\n","        self.header_printed = True\n","\n","    def __start_progress(self, maxi):\n","        def print_progress():\n","            # Bresenham's algorithm. Yields the number of dots printed.\n","            # This will always print 100 dots in max invocations.\n","            dx = maxi\n","            dy = self.size\n","            d = dy - dx\n","            for x in range(maxi):\n","                k = 0\n","                while d >= 0:\n","                    print('=', end=\"\", flush=True)\n","                    k += 1\n","                    d -= dx\n","                d += dy\n","                yield k\n","\n","        return print_progress\n","\n","\n","def read_data_files(directory, validation=True):\n","    \"\"\"Read data files according to the specified glob pattern\n","    Optionnaly set aside the last file as validation data.\n","    No validation data is returned if there are 5 files or less.\n","    :param directory: for example \"data/*.txt\"\n","    :param validation: if True (default), sets the last file aside as validation data\n","    :return: training data, validation data, list of loaded file names with ranges\n","     If validation is\n","    \"\"\"\n","    codetext = []\n","    bookranges = []\n","    shakelist = glob.glob(directory, recursive=True)\n","    for shakefile in shakelist:\n","        shaketext = open(shakefile, \"r\")\n","        print(\"Loading file \" + shakefile)\n","        start = len(codetext)\n","        codetext.extend(encode_text(shaketext.read()))\n","        end = len(codetext)\n","        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n","        shaketext.close()\n","\n","    if len(bookranges) == 0:\n","        sys.exit(\"No training data has been found. Aborting.\")\n","\n","    # For validation, use roughly 90K of text,\n","    # but no more than 10% of the entire text\n","    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n","\n","    # 10% of the text is how many files ?\n","    total_len = len(codetext)\n","    validation_len = 0\n","    nb_books1 = 0\n","    for book in reversed(bookranges):\n","        validation_len += book[\"end\"]-book[\"start\"]\n","        nb_books1 += 1\n","        if validation_len > total_len // 10:\n","            break\n","\n","    # 90K of text is how many books ?\n","    validation_len = 0\n","    nb_books2 = 0\n","    for book in reversed(bookranges):\n","        validation_len += book[\"end\"]-book[\"start\"]\n","        nb_books2 += 1\n","        if validation_len > 90*1024:\n","            break\n","\n","    # 20% of the books is how many books ?\n","    nb_books3 = len(bookranges) // 5\n","\n","    # pick the smallest\n","    nb_books = min(nb_books1, nb_books2, nb_books3)\n","\n","    if nb_books == 0 or not validation:\n","        cutoff = len(codetext)\n","    else:\n","        cutoff = bookranges[-nb_books][\"start\"]\n","    valitext = codetext[cutoff:]\n","    codetext = codetext[:cutoff]\n","    return codetext, valitext, bookranges\n","\n","\n","def print_data_stats(datalen, valilen, epoch_size):\n","    datalen_mb = datalen/1024.0/1024.0\n","    valilen_kb = valilen/1024.0\n","    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n","          + \" There will be {} batches per epoch\".format(epoch_size))\n","\n","\n","def print_validation_header(validation_start, bookranges):\n","    bookindex = find_book_index(validation_start, bookranges)\n","    books = ''\n","    for i in range(bookindex, len(bookranges)):\n","        books += bookranges[i][\"name\"]\n","        if i < len(bookranges)-1:\n","            books += \", \"\n","    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n","\n","\n","def print_validation_stats(loss, accuracy):\n","    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n","                                                                                                           accuracy))\n","\n","\n","def print_text_generation_header():\n","    print()\n","    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n","\n","\n","def print_text_generation_footer():\n","    print()\n","    print(\"└{:─^111}┘\".format('End of generation'))\n","\n","\n","def frequency_limiter(n, multiple=1, modulo=0):\n","    def limit(i):\n","        return i % (multiple * n) == modulo*multiple\n","    return limit\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jFisR4XGTH1p","colab_type":"text"},"cell_type":"markdown","source":["### Training the Model"]},{"metadata":{"id":"j9BSXtgQPF0H","colab_type":"code","colab":{}},"cell_type":"code","source":["# encoding: UTF-8\n","# Copyright 2017 Google.com\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","import importlib.util\n","import tensorflow as tf\n","from tensorflow.contrib import layers\n","from tensorflow.contrib import rnn  # rnn stuff temporarily in contrib, moving back to code in TF 1.1\n","import os\n","import time\n","import math\n","import numpy as np\n","tf.set_random_seed(0)\n","\n","# model parameters\n","#\n","# Usage:\n","#   Training only:\n","#         Leave all the parameters as they are\n","#         Disable validation to run a bit faster (set validation=False below)\n","#         You can follow progress in Tensorboard: tensorboard --log-dir=log\n","#   Training and experimentation (default):\n","#         Keep validation enabled\n","#         You can now play with the parameters anf follow the effects in Tensorboard\n","#         A good choice of parameters ensures that the testing and validation curves stay close\n","#         To see the curves drift apart (\"overfitting\") try to use an insufficient amount of\n","#         training data (shakedir = \"shakespeare/t*.txt\" for example)\n","#\n","SEQLEN = 30\n","BATCHSIZE = 200\n","ALPHASIZE = ALPHASIZE\n","INTERNALSIZE = 512\n","NLAYERS = 3\n","learning_rate = 0.001  # fixed learning rate\n","dropout_pkeep = 0.8  # some dropout\n"," \n","# load data, either shakespeare, or the Python source of Tensorflow itself\n","default_fp = 'drive/Colab Notebooks/ex03/'\n","shakedir = default_fp + \"/shakespeare/*.txt\"\n","#shakedir = \"../tensorflow/**/*.py\"\n","codetext, valitext, bookranges = read_data_files(shakedir, validation=True)\n","\n","# display some stats on the data\n","epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n","print_data_stats(len(codetext), len(valitext), epoch_size)\n","\n","#\n","# the model (see FAQ in README.md)\n","#\n","lr = tf.placeholder(tf.float32, name='lr')  # learning rate\n","pkeep = tf.placeholder(tf.float32, name='pkeep')  # dropout parameter\n","batchsize = tf.placeholder(tf.int32, name='batchsize')\n","\n","# inputs\n","X = tf.placeholder(tf.uint8, [None, None], name='X')    # [ BATCHSIZE, SEQLEN ]\n","Xo = tf.one_hot(X, ALPHASIZE, 1.0, 0.0)                 # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n","# expected outputs = same sequence shifted by 1 since we are trying to predict the next character\n","Y_ = tf.placeholder(tf.uint8, [None, None], name='Y_')  # [ BATCHSIZE, SEQLEN ]\n","Yo_ = tf.one_hot(Y_, ALPHASIZE, 1.0, 0.0)               # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n","# input state\n","Hin = tf.placeholder(tf.float32, [None, INTERNALSIZE*NLAYERS], name='Hin')  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n","\n","# using a NLAYERS=3 layers of GRU cells, unrolled SEQLEN=30 times\n","# dynamic_rnn infers SEQLEN from the size of the inputs Xo\n","\n","# How to properly apply dropout in RNNs: see README.md\n","cells = [rnn.GRUCell(INTERNALSIZE) for _ in range(NLAYERS)]\n","# \"naive dropout\" implementation\n","dropcells = [rnn.DropoutWrapper(cell,input_keep_prob=pkeep) for cell in cells]\n","multicell = rnn.MultiRNNCell(dropcells, state_is_tuple=False)\n","multicell = rnn.DropoutWrapper(multicell, output_keep_prob=pkeep)  # dropout for the softmax layer\n","\n","Yr, H = tf.nn.dynamic_rnn(multicell, Xo, dtype=tf.float32, initial_state=Hin)\n","# Yr: [ BATCHSIZE, SEQLEN, INTERNALSIZE ]\n","# H:  [ BATCHSIZE, INTERNALSIZE*NLAYERS ] # this is the last state in the sequence\n","\n","H = tf.identity(H, name='H')  # just to give it a name\n","\n","# Softmax layer implementation:\n","# Flatten the first two dimension of the output [ BATCHSIZE, SEQLEN, ALPHASIZE ] => [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n","# then apply softmax readout layer. This way, the weights and biases are shared across unrolled time steps.\n","# From the readout point of view, a value coming from a sequence time step or a minibatch item is the same thing.\n","\n","Yflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ]\n","Ylogits = layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n","Yflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n","loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]\n","loss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ]\n","Yo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n","Y = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ]\n","Y = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ]\n","\n","# choose the optimizer\n","# train_step = tf.train.AdamOptimizer(lr)\n","train_step = tf.train.RMSPropOptimizer(lr, momentum=0.9)\n","# train_step = tf.train.GradientDescentOptimizer(lr)\n","name = train_step.get_name()\n","train_step = train_step.minimize(loss)\n","\n","# stats for display\n","seqloss = tf.reduce_mean(loss, 1)\n","batchloss = tf.reduce_mean(seqloss)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(Y, tf.uint8)), tf.float32))\n","loss_summary = tf.summary.scalar(\"batch_loss\", batchloss)\n","acc_summary = tf.summary.scalar(\"batch_accuracy\", accuracy)\n","summaries = tf.summary.merge([loss_summary, acc_summary])\n","\n","# Init Tensorboard stuff. This will save Tensorboard information into a different\n","# folder at each run named 'log/<timestamp>/'. Two sets of data are saved so that\n","# you can compare training and validation curves visually in Tensorboard.\n","timestamp = str(math.trunc(time.time()))\n","summary_writer = tf.summary.FileWriter(default_fp + \"/log/\" + timestamp + '-{}'.format(name) + \"-training\", flush_secs=15)\n","validation_writer = tf.summary.FileWriter(default_fp + \"/log/\" + timestamp + '-{}'.format(name) + \"-validation\", flush_secs=15)\n","\n","# Init for saving models. They will be saved into a directory named 'checkpoints'.\n","# Only the last checkpoint is kept.\n","if not os.path.exists(\"checkpoints\"):\n","    os.mkdir(\"checkpoints\")\n","saver = tf.train.Saver(max_to_keep=1000)\n","\n","# for display: init the progress bar\n","DISPLAY_FREQ = 50\n","_50_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\n","progress = Progress(DISPLAY_FREQ, size=111+2, msg=\"Training on next \"+str(DISPLAY_FREQ)+\" batches\")\n","\n","# init\n","istate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS])  # initial zero input state\n","init = tf.global_variables_initializer()\n","sess = tf.Session()\n","sess.run(init)\n","step = 0\n","\n","# training loop\n","for x, y_, epoch in rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=4):\n","\n","    # train on one minibatch\n","    feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n","    _, y, ostate = sess.run([train_step, Y, H], feed_dict=feed_dict)\n","\n","    # log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\n","    if step % _50_BATCHES == 0:\n","        feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n","        y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n","        print_learning_learned_comparison(x, y, l, bookranges, bl, acc, epoch_size, step, epoch)\n","        summary_writer.add_summary(smm, step)\n","        summary_writer.flush()\n","\n","    # run a validation step every 50 batches\n","    # The validation text should be a single sequence but that's too slow (1s per 1024 chars!),\n","    # so we cut it up and batch the pieces (slightly inaccurate)\n","    # tested: validating with 5K sequences instead of 1K is only slightly more accurate, but a lot slower.\n","    if step % _50_BATCHES == 0 and len(valitext) > 0:\n","        VALI_SEQLEN = 1*1024  # Sequence length for validation. State will be wrong at the start of each sequence.\n","        bsize = len(valitext) // VALI_SEQLEN\n","        print_validation_header(len(codetext), bookranges)\n","        vali_x, vali_y, _ = next(rnn_minibatch_sequencer(valitext, bsize, VALI_SEQLEN, 1))  # all data in 1 batch\n","        vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n","        feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0,  # no dropout for validation\n","                     batchsize: bsize}\n","        ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n","        print_validation_stats(ls, acc)\n","        # save validation data for Tensorboard\n","        validation_writer.add_summary(smm, step)\n","        validation_writer.flush()\n","\n","#     # display a short text generated with the current weights and biases (every 500 batches)\n","#     if step // 10 % _50_BATCHES == 0:\n","#         print_text_generation_header()\n","#         ry = np.array([[convert_from_alphabet(ord(\"K\"))]])\n","#         rh = np.zeros([1, INTERNALSIZE * NLAYERS])\n","#         for k in range(1000):\n","#             ryo, rh = sess.run([Yo, H], feed_dict={X: ry, pkeep: 1.0, Hin: rh, batchsize: 1})\n","#             rc = sample_from_probabilities(ryo, topn=10 if epoch <= 1 else 2)\n","#             print(chr(convert_to_alphabet(rc)), end=\"\")\n","#             ry = np.array([[rc]])\n","#         print_text_generation_footer()\n","\n","#     # save a checkpoint (every 500 batches)\n","#     if step // 10 % _50_BATCHES == 0:\n","#         saved_file = saver.save(sess, default_fp + '/checkpoints/rnn_train_' + timestamp, global_step=step)\n","#         print(\"Saved file: \" + saved_file)\n","\n","    # display progress bar\n","    progress.step(reset=step % _50_BATCHES == 0)\n","\n","    # loop state around\n","    istate = ostate\n","    step += BATCHSIZE * SEQLEN\n","\n","# all runs: SEQLEN = 30, BATCHSIZE = 100, ALPHASIZE = 98, INTERNALSIZE = 512, NLAYERS = 3\n","# run 1477669632 decaying learning rate 0.001-0.0001-1e7 dropout 0.5: not good\n","# run 1477670023 lr=0.001 no dropout: very good\n","\n","# Tensorflow runs:\n","# 1485434262\n","#   trained on shakespeare/t*.txt only. Validation on 1K sequences\n","#   validation loss goes up from step 5M (overfitting because of small dataset)\n","# 1485436038\n","#   trained on shakespeare/t*.txt only. Validation on 5K sequences\n","#   On 5K sequences validation accuracy is slightly higher and loss slightly lower\n","#   => sequence breaks do introduce inaccuracies but the effect is small\n","# 1485437956\n","#   Trained on shakespeare/*.txt. Validation on 1K sequences\n","#   On this much larger dataset, validation loss still decreasing after 6 epochs (step 35M)\n","# 1495447371\n","#   Trained on shakespeare/*.txt no dropout, 30 epochs\n","#   Validation loss starts going up after 10 epochs (overfitting)\n","# 1495440473\n","#   Trained on shakespeare/*.txt \"naive dropout\" pkeep=0.8, 30 epochs\n","#   Dropout brings the validation loss under control, preventing it from\n","#   going up but the effect is small.\n"],"execution_count":0,"outputs":[]}]}