{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ex02_Moritz_Eck.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fPDfqEoJzqrb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning for Natural Language Processing: Exercise 02\n",
        "Moritz Eck (14-715-296)<br/>\n",
        "University of Zurich\n",
        "\n",
        "Please see the section right at the bottom of this notebook for the discussion of the results as well as the answers to the exercise questions."
      ]
    },
    {
      "metadata": {
        "id": "xNvqfpAIIRjB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive (Please do this step first => only needs to be done once!)\n",
        "\n",
        "This mounts the user's Google Drive directly.\n",
        "\n",
        "On my personal machine inside the Google Drive folder the input files are stored in the following folder:<br/> \n",
        "**~/Google Drive/Colab Notebooks/ex02/**\n",
        "\n",
        "Below I have defined the default filepath as **default_fp = 'drive/Colab Notebooks/ex02/'**.<br/>\n",
        "Please change the filepath to the location where you have the input file and the glove embeddings saved."
      ]
    },
    {
      "metadata": {
        "id": "LQdGNRs3H76k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "\n",
        "import getpass\n",
        "\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5_E4VnfSwO6P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Mount Google Drive**"
      ]
    },
    {
      "metadata": {
        "id": "j9CGOGBLMIrR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Omwg_12FItFI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install the required packages"
      ]
    },
    {
      "metadata": {
        "id": "x6RuPbebIudp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install pandas==0.23.4\n",
        "!pip install numpy \n",
        "!pip install scikit-learn==0.20\n",
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wzB5plD1I8bw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Check that the GPU is used"
      ]
    },
    {
      "metadata": {
        "id": "PT7pCqDHIvQF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "3b5cf50e-df7f-4255-93d6-99d9dbb536d1"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 17563149522823612193\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11281553818\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 11908094527757943913\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9AHoHfqQJi_B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing TED Talks File\n",
        "- Run Once"
      ]
    },
    {
      "metadata": {
        "id": "FrefaRtMJrUy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "4dbbba79-381c-454d-c463-f8309e7f94af"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 200\n",
        "seed = np.random.seed(seed)\n",
        "\n",
        "# default inputs filepath\n",
        "default_fp = 'drive/Colab Notebooks/ex02/'\n",
        "ted_file = default_fp + './inputs/ted_en-20160408.xml'\n",
        "\n",
        "# constants\n",
        "lines = []\n",
        "talks = {}\n",
        "relevant = False\n",
        "talk = \"\"\n",
        "key = \"\"\n",
        "talk_count = 0\n",
        "\n",
        "print(\"Preprocessing TED Talks...\")\n",
        "\n",
        "with open(ted_file, 'r', encoding='utf-8') as freader:\n",
        "    lines = freader.readlines()\n",
        "\n",
        "for line in lines:\n",
        "    # determine according to the keywords if the talk is relevant\n",
        "    if '<keywords' in line:\n",
        "        key = \"\"\n",
        "\n",
        "        if 'technology' in line:\n",
        "            key += \"T\"\n",
        "        else:\n",
        "            key += 'x'          \n",
        "\n",
        "        if 'entertainment' in line:\n",
        "            key += \"E\"\n",
        "        else:\n",
        "            key += 'x'  \n",
        "\n",
        "        if 'design' in line:\n",
        "            key += 'D'\n",
        "        else:\n",
        "            key += 'x'\n",
        "\n",
        "        if key != 'xxx':\n",
        "            relevant = True\n",
        "            continue\n",
        "        else:\n",
        "            relevant = False\n",
        "            continue\n",
        "\n",
        "    if not relevant:\n",
        "        continue\n",
        "    \n",
        "    # start reading the content\n",
        "    if '<transcription>' in line:\n",
        "        talk = \"\"\n",
        "\n",
        "    # append each line of the transcript\n",
        "    elif '<seekvideo' in line:\n",
        "        start = line.find('>') + 1\n",
        "        end = line.rfind('<') \n",
        "        talk += line[start:end] + \" \"\n",
        "\n",
        "    # end of a talk\n",
        "    elif '</transcription>' in line: \n",
        "        # store each talk with key and content\n",
        "        talks[talk_count] = [key, talk]\n",
        "        talk_count += 1\n",
        "        relevant = False\n",
        "\n",
        "# transform the dict into a dataframe\n",
        "df = pd.DataFrame.from_dict(talks, orient='index')\n",
        "df = df.rename(index=str, columns={0:'label', 1:'talk'})\n",
        "df = df.reset_index().drop(columns=['index'])\n",
        "print(df.describe())\n",
        "\n",
        "# link to pretrained embeddings tutorial\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "# https://fasttext.cc/docs/en/english-vectors.html\n",
        "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing TED Talks...\n",
            "       label                                               talk\n",
            "count    957                                                957\n",
            "unique     7                                                957\n",
            "top      Txx  How would you like to be better than you are? ...\n",
            "freq     381                                                  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G_Bh7m0lTSO5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Pre-Trained Word Embeddings\n",
        "- Run Once\n",
        "- If the embedding dimension shall be changed => change the filepath to the respective 60d/100d/200d/300d.Then re-run!"
      ]
    },
    {
      "metadata": {
        "id": "mV8Hq1yNO_P2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ed893722-6058-47fd-c382-b100c89e6c68"
      },
      "cell_type": "code",
      "source": [
        "print('Indexing word vectors.')\n",
        "\n",
        "glove_file = default_fp + './glove/glove.6B.100d.txt'\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "with open(glove_file, encoding='utf-8') as freader:\n",
        "    for line in freader:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d-hktXgW7qfD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Basic Sklearn MLP\n"
      ]
    },
    {
      "metadata": {
        "id": "eY6gvvR37rp0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# get the preprocessed data\n",
        "df = df.copy()\n",
        "\n",
        "# compute the class weight\n",
        "class_weights = compute_class_weight('balanced', np.unique(df['label']), df['label'])\n",
        "\n",
        "for label, weight in zip(np.unique(df['label']), class_weights):\n",
        "    print(\"label: {} -> weight: {}\".format(label, weight))\n",
        "\n",
        "# split in training and test set\n",
        "train = df.sample(frac=0.8, random_state=seed)\n",
        "test = df.drop(train.index)\n",
        "\n",
        "# for training\n",
        "y_train = train['label']\n",
        "x_train = train.drop('label', axis=1)\n",
        "x_train = x_train['talk'].values\n",
        "\n",
        "# for testing\n",
        "y_test = test['label']\n",
        "x_test = test.drop('label', axis=1)\n",
        "x_test = x_test['talk'].values\n",
        "\n",
        "print('Training samples shape: ', x_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test samples shape: ', x_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)\n",
        "\n",
        "# encode the label\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "print(label_encoder.classes_)\n",
        "\n",
        "# transform the talks\n",
        "tfvect = TfidfVectorizer(ngram_range=(1,3), max_df=0.99, max_features=None)\n",
        "tfvect.fit(x_train)\n",
        "x_train = tfvect.transform(x_train)\n",
        "x_test = tfvect.transform(x_test)\n",
        "\n",
        "# standardize the data\n",
        "stand = StandardScaler(with_mean=False, with_std=True)\n",
        "stand.fit(x_train)\n",
        "x_train = stand.transform(x_train)\n",
        "x_test = stand.transform(x_test)\n",
        "\n",
        "# setup base mlp classifier\n",
        "mlp = MLPClassifier(early_stopping=True, validation_fraction=0.2, random_state=seed, batch_size='auto', \n",
        "                    max_iter=200, n_iter_no_change=15, learning_rate='adaptive', verbose=True)\n",
        "\n",
        "# set up parameter grid to evaluate over\n",
        "param_grid = dict(hidden_layer_sizes=[(100,100,100), (100,100), (100,)], \n",
        "                  solver = ['adam', 'sgd'], activation=['tanh', 'relu'], alpha=[0.001, 0.0001])\n",
        "\n",
        "# train mlp classifier using randomized grid search\n",
        "gs_mlp = RandomizedSearchCV(mlp, param_grid, n_iter=5, cv=5, n_jobs=4, verbose=True, refit=True)\n",
        "gs_mlp.fit(x_train, y_train)\n",
        "\n",
        "# print the best parameters of the evaluation\n",
        "print(gs_mlp.best_params_)\n",
        "print(gs_mlp.best_score_)\n",
        "\n",
        "# predict the test label\n",
        "y_pred = gs_mlp.predict(x_test)\n",
        "\n",
        "# print the accuracy and the confusion matrix\n",
        "print(accuracy_score(y_pred, y_test))\n",
        "print(confusion_matrix(y_pred, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T0J6vzkXT-Nd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Keras Model\n",
        "Run only once or once the model input parameters have been change: \n",
        "- e.g. changing input/embedding dimensions, MAX_NUM_WORDS, MAX_SEQUENCE_LENGTH"
      ]
    },
    {
      "metadata": {
        "id": "Lvdle6a-T_rq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten, Dropout\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD, Adam, Adagrad\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# constants\n",
        "MAX_SEQUENCE_LENGTH = 2000\n",
        "MAX_NUM_WORDS = 10000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# input data\n",
        "texts = df['talk'].values\n",
        "labels = df['label'].values\n",
        "\n",
        "# compute the class weight\n",
        "classes = np.unique(df['label'])\n",
        "class_weights = compute_class_weight('balanced', classes, df['label'])\n",
        "class_weights = {label:weight for label, weight in zip(range(len(classes)), class_weights)}\n",
        "\n",
        "# encode the label as numerical value\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, \n",
        "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
        "                      lower=True, split=' ', char_level=False)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# one-hot encode the labels\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# split the data into a training, validation and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, stratify=labels, \n",
        "                                                    test_size=VALIDATION_SPLIT, shuffle=True)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, stratify=y_train, \n",
        "                                                  test_size=VALIDATION_SPLIT, shuffle=True)\n",
        "\n",
        "print('Preparing embedding matrix.')\n",
        "\n",
        "# prepare embedding matrix\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-YviO6PBFKuX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model Training\n",
        "- Change the parameters\n",
        "- Rerun with new configurations"
      ]
    },
    {
      "metadata": {
        "id": "b-j3YzGBAQlF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4042
        },
        "outputId": "118c6c54-adbc-4325-8027-ab016f0bec5e"
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten, Dropout\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD, Adam, Adagrad\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# load pre-trained word embeddings into an embedding layer\n",
        "# if trainable = False, it will keep the embeddings fixed\n",
        "embedding_layer = Embedding(input_dim=num_words, output_dim=EMBEDDING_DIM, \n",
        "                            embeddings_initializer=Constant(embedding_matrix), \n",
        "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
        "\n",
        "print('Training model.')\n",
        "\n",
        "# create a NN using a pre-trained embedding layer\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "# flatten the embedding matrix to use it in a feed forward nn\n",
        "x = Conv1D(filters=128, kernel_size=10, padding='same', activation='relu')(embedded_sequences)\n",
        "# x = Flatten()(embedded_sequences)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "\n",
        "# hidden layers\n",
        "x = Dense(128, activation='relu', kernel_regularizer=l2(0.1))(x)\n",
        "x = Dense(128, activation='relu', kernel_regularizer=l2(0.2))(x)\n",
        "x = Dense(128, activation='relu', kernel_regularizer=l2(0.3))(x)\n",
        "\n",
        "# output layer with softmax activation\n",
        "preds = Dense(len(classes), activation='softmax')(x)\n",
        "\n",
        "# different optimizers\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) \n",
        "adagrad = Adagrad()\n",
        "adam = Adam()\n",
        "\n",
        "for opt in [adam, adagrad, sgd]:\n",
        "    # build, compile and print summary of the current model\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc', 'mae'])\n",
        "    print(model.summary())\n",
        "    \n",
        "    # stop training if paramter stops improving for x epochs\n",
        "    callbacks = [EarlyStopping(patience=25, monitor='val_acc')]\n",
        "\n",
        "    # fit the model\n",
        "    model.fit(x_train, y_train, batch_size=128, epochs=100, \n",
        "              validation_data=(x_val, y_val), callbacks=callbacks,\n",
        "              class_weight=class_weights)\n",
        "\n",
        "    # evaluate the model on the test set\n",
        "    y_pred = model.predict(x_test)\n",
        "    \n",
        "    # convert one hot back to categorical\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    y_test_ = np.argmax(y_test, axis=1)\n",
        "    \n",
        "    # evaluate the model using the test set\n",
        "    print(accuracy_score(y_pred, y_test_))\n",
        "    print(label_encoder.classes_)\n",
        "    print(class_weights)\n",
        "    print(confusion_matrix(y_pred, y_test_))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_32 (InputLayer)        (None, 2000)              0         \n",
            "_________________________________________________________________\n",
            "embedding_32 (Embedding)     (None, 2000, 100)         1000100   \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 2000, 128)         128128    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_21 (Glo (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_103 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_104 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_105 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 1,178,667\n",
            "Trainable params: 178,567\n",
            "Non-trainable params: 1,000,100\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 612 samples, validate on 153 samples\n",
            "Epoch 1/100\n",
            "612/612 [==============================] - 5s 8ms/step - loss: 76.5461 - acc: 0.1928 - mean_absolute_error: 0.2431 - val_loss: 72.4970 - val_acc: 0.2026 - val_mean_absolute_error: 0.2444\n",
            "Epoch 2/100\n",
            "612/612 [==============================] - 0s 806us/step - loss: 69.8568 - acc: 0.4379 - mean_absolute_error: 0.2407 - val_loss: 65.8567 - val_acc: 0.3856 - val_mean_absolute_error: 0.2416\n",
            "Epoch 3/100\n",
            "612/612 [==============================] - 0s 800us/step - loss: 63.3234 - acc: 0.5147 - mean_absolute_error: 0.2392 - val_loss: 59.6690 - val_acc: 0.2614 - val_mean_absolute_error: 0.2427\n",
            "Epoch 4/100\n",
            "612/612 [==============================] - 0s 814us/step - loss: 57.2140 - acc: 0.6422 - mean_absolute_error: 0.2342 - val_loss: 53.7839 - val_acc: 0.4575 - val_mean_absolute_error: 0.2360\n",
            "Epoch 5/100\n",
            "612/612 [==============================] - 0s 805us/step - loss: 51.5681 - acc: 0.6307 - mean_absolute_error: 0.2315 - val_loss: 48.5914 - val_acc: 0.2876 - val_mean_absolute_error: 0.2403\n",
            "Epoch 6/100\n",
            "612/612 [==============================] - 0s 813us/step - loss: 46.4231 - acc: 0.5866 - mean_absolute_error: 0.2312 - val_loss: 43.7493 - val_acc: 0.4510 - val_mean_absolute_error: 0.2385\n",
            "Epoch 7/100\n",
            "612/612 [==============================] - 0s 798us/step - loss: 41.7356 - acc: 0.6846 - mean_absolute_error: 0.2294 - val_loss: 39.3230 - val_acc: 0.5098 - val_mean_absolute_error: 0.2341\n",
            "Epoch 8/100\n",
            "612/612 [==============================] - 0s 801us/step - loss: 37.4965 - acc: 0.6699 - mean_absolute_error: 0.2243 - val_loss: 35.4597 - val_acc: 0.3268 - val_mean_absolute_error: 0.2383\n",
            "Epoch 9/100\n",
            "612/612 [==============================] - 0s 806us/step - loss: 33.6576 - acc: 0.7418 - mean_absolute_error: 0.2254 - val_loss: 31.7909 - val_acc: 0.5033 - val_mean_absolute_error: 0.2315\n",
            "Epoch 10/100\n",
            "612/612 [==============================] - 0s 810us/step - loss: 30.1963 - acc: 0.6846 - mean_absolute_error: 0.2204 - val_loss: 28.6885 - val_acc: 0.4052 - val_mean_absolute_error: 0.2369\n",
            "Epoch 11/100\n",
            "612/612 [==============================] - 0s 810us/step - loss: 27.0848 - acc: 0.7092 - mean_absolute_error: 0.2198 - val_loss: 25.7280 - val_acc: 0.5294 - val_mean_absolute_error: 0.2322\n",
            "Epoch 12/100\n",
            "612/612 [==============================] - 0s 807us/step - loss: 24.2782 - acc: 0.8007 - mean_absolute_error: 0.2169 - val_loss: 23.1438 - val_acc: 0.4967 - val_mean_absolute_error: 0.2319\n",
            "Epoch 13/100\n",
            "612/612 [==============================] - 0s 809us/step - loss: 21.7707 - acc: 0.6225 - mean_absolute_error: 0.2171 - val_loss: 20.8914 - val_acc: 0.3529 - val_mean_absolute_error: 0.2352\n",
            "Epoch 14/100\n",
            "612/612 [==============================] - 0s 816us/step - loss: 19.5148 - acc: 0.7353 - mean_absolute_error: 0.2149 - val_loss: 18.7100 - val_acc: 0.4510 - val_mean_absolute_error: 0.2281\n",
            "Epoch 15/100\n",
            "612/612 [==============================] - 0s 809us/step - loss: 17.4982 - acc: 0.6912 - mean_absolute_error: 0.2118 - val_loss: 16.9250 - val_acc: 0.3987 - val_mean_absolute_error: 0.2319\n",
            "Epoch 16/100\n",
            "612/612 [==============================] - 1s 825us/step - loss: 15.6819 - acc: 0.6618 - mean_absolute_error: 0.2102 - val_loss: 15.2056 - val_acc: 0.3987 - val_mean_absolute_error: 0.2293\n",
            "Epoch 17/100\n",
            "612/612 [==============================] - 0s 814us/step - loss: 14.0492 - acc: 0.7042 - mean_absolute_error: 0.2083 - val_loss: 13.7439 - val_acc: 0.3268 - val_mean_absolute_error: 0.2305\n",
            "Epoch 18/100\n",
            "612/612 [==============================] - 0s 815us/step - loss: 12.6027 - acc: 0.7173 - mean_absolute_error: 0.2063 - val_loss: 12.3493 - val_acc: 0.5033 - val_mean_absolute_error: 0.2254\n",
            "Epoch 19/100\n",
            "612/612 [==============================] - 1s 819us/step - loss: 11.3144 - acc: 0.6438 - mean_absolute_error: 0.2044 - val_loss: 11.1985 - val_acc: 0.3203 - val_mean_absolute_error: 0.2276\n",
            "Epoch 20/100\n",
            "612/612 [==============================] - 0s 806us/step - loss: 10.1492 - acc: 0.6405 - mean_absolute_error: 0.2017 - val_loss: 10.1065 - val_acc: 0.5098 - val_mean_absolute_error: 0.2244\n",
            "Epoch 21/100\n",
            "612/612 [==============================] - 0s 814us/step - loss: 9.1278 - acc: 0.7337 - mean_absolute_error: 0.2013 - val_loss: 9.1701 - val_acc: 0.4510 - val_mean_absolute_error: 0.2234\n",
            "Epoch 22/100\n",
            "612/612 [==============================] - 0s 812us/step - loss: 8.2224 - acc: 0.7598 - mean_absolute_error: 0.1997 - val_loss: 8.3823 - val_acc: 0.4118 - val_mean_absolute_error: 0.2274\n",
            "Epoch 23/100\n",
            "612/612 [==============================] - 1s 819us/step - loss: 7.4023 - acc: 0.7222 - mean_absolute_error: 0.1974 - val_loss: 7.5888 - val_acc: 0.4967 - val_mean_absolute_error: 0.2235\n",
            "Epoch 24/100\n",
            "612/612 [==============================] - 0s 811us/step - loss: 6.6834 - acc: 0.6601 - mean_absolute_error: 0.1984 - val_loss: 6.9010 - val_acc: 0.4248 - val_mean_absolute_error: 0.2194\n",
            "Epoch 25/100\n",
            "612/612 [==============================] - 0s 815us/step - loss: 6.0497 - acc: 0.6046 - mean_absolute_error: 0.1938 - val_loss: 6.4487 - val_acc: 0.2484 - val_mean_absolute_error: 0.2302\n",
            "Epoch 26/100\n",
            "612/612 [==============================] - 0s 809us/step - loss: 5.4809 - acc: 0.7092 - mean_absolute_error: 0.1958 - val_loss: 5.7988 - val_acc: 0.4837 - val_mean_absolute_error: 0.2174\n",
            "Epoch 27/100\n",
            "612/612 [==============================] - 0s 806us/step - loss: 4.9675 - acc: 0.7565 - mean_absolute_error: 0.1916 - val_loss: 5.4573 - val_acc: 0.4248 - val_mean_absolute_error: 0.2274\n",
            "Epoch 28/100\n",
            "612/612 [==============================] - 0s 797us/step - loss: 4.5313 - acc: 0.6846 - mean_absolute_error: 0.1920 - val_loss: 4.9297 - val_acc: 0.4575 - val_mean_absolute_error: 0.2166\n",
            "Epoch 29/100\n",
            "612/612 [==============================] - 0s 806us/step - loss: 4.1299 - acc: 0.6569 - mean_absolute_error: 0.1903 - val_loss: 4.6063 - val_acc: 0.4575 - val_mean_absolute_error: 0.2208\n",
            "Epoch 30/100\n",
            "612/612 [==============================] - 0s 797us/step - loss: 3.7789 - acc: 0.7141 - mean_absolute_error: 0.1872 - val_loss: 4.3182 - val_acc: 0.3987 - val_mean_absolute_error: 0.2212\n",
            "Epoch 31/100\n",
            "612/612 [==============================] - 0s 806us/step - loss: 3.4731 - acc: 0.7549 - mean_absolute_error: 0.1869 - val_loss: 4.0269 - val_acc: 0.4510 - val_mean_absolute_error: 0.2184\n",
            "Epoch 32/100\n",
            "612/612 [==============================] - 0s 806us/step - loss: 3.2023 - acc: 0.6781 - mean_absolute_error: 0.1853 - val_loss: 3.7880 - val_acc: 0.2941 - val_mean_absolute_error: 0.2191\n",
            "Epoch 33/100\n",
            "612/612 [==============================] - 0s 812us/step - loss: 2.9557 - acc: 0.5833 - mean_absolute_error: 0.1842 - val_loss: 3.5498 - val_acc: 0.3922 - val_mean_absolute_error: 0.2169\n",
            "Epoch 34/100\n",
            "612/612 [==============================] - 0s 805us/step - loss: 2.7381 - acc: 0.7304 - mean_absolute_error: 0.1797 - val_loss: 3.3615 - val_acc: 0.4444 - val_mean_absolute_error: 0.2165\n",
            "Epoch 35/100\n",
            "612/612 [==============================] - 0s 812us/step - loss: 2.5519 - acc: 0.7761 - mean_absolute_error: 0.1783 - val_loss: 3.1921 - val_acc: 0.4771 - val_mean_absolute_error: 0.2150\n",
            "Epoch 36/100\n",
            "612/612 [==============================] - 0s 804us/step - loss: 2.3871 - acc: 0.5784 - mean_absolute_error: 0.1789 - val_loss: 3.0671 - val_acc: 0.2484 - val_mean_absolute_error: 0.2152\n",
            "0.234375\n",
            "['TED' 'TEx' 'TxD' 'Txx' 'xED' 'xEx' 'xxD']\n",
            "{0: 4.0210084033613445, 1: 3.7976190476190474, 2: 0.9428571428571428, 3: 0.35883014623172105, 4: 5.696428571428571, 5: 0.7902559867877786, 6: 0.8336236933797909}\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  1  0]\n",
            " [ 4  4 23 66  1  2 20]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  2  1 11  2]\n",
            " [ 3  3  6  8  3 21 11]]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_32 (InputLayer)        (None, 2000)              0         \n",
            "_________________________________________________________________\n",
            "embedding_32 (Embedding)     (None, 2000, 100)         1000100   \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 2000, 128)         128128    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_21 (Glo (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_103 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_104 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_105 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 1,178,667\n",
            "Trainable params: 178,567\n",
            "Non-trainable params: 1,000,100\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 612 samples, validate on 153 samples\n",
            "Epoch 1/100\n",
            "612/612 [==============================] - 5s 8ms/step - loss: 6.1183 - acc: 0.1912 - mean_absolute_error: 0.2422 - val_loss: 2.4413 - val_acc: 0.0327 - val_mean_absolute_error: 0.2426\n",
            "Epoch 2/100\n",
            "612/612 [==============================] - 0s 792us/step - loss: 2.2706 - acc: 0.1683 - mean_absolute_error: 0.2398 - val_loss: 2.4604 - val_acc: 0.0261 - val_mean_absolute_error: 0.2478\n",
            "Epoch 3/100\n",
            "612/612 [==============================] - 0s 793us/step - loss: 2.1223 - acc: 0.1944 - mean_absolute_error: 0.2409 - val_loss: 2.1461 - val_acc: 0.3725 - val_mean_absolute_error: 0.2370\n",
            "Epoch 4/100\n",
            "612/612 [==============================] - 0s 802us/step - loss: 1.9090 - acc: 0.4346 - mean_absolute_error: 0.2311 - val_loss: 2.7972 - val_acc: 0.0327 - val_mean_absolute_error: 0.2536\n",
            "Epoch 5/100\n",
            "612/612 [==============================] - 0s 794us/step - loss: 2.2555 - acc: 0.2010 - mean_absolute_error: 0.2456 - val_loss: 2.1170 - val_acc: 0.4379 - val_mean_absolute_error: 0.2384\n",
            "Epoch 6/100\n",
            "612/612 [==============================] - 0s 792us/step - loss: 1.8362 - acc: 0.5098 - mean_absolute_error: 0.2305 - val_loss: 2.1797 - val_acc: 0.1765 - val_mean_absolute_error: 0.2392\n",
            "Epoch 7/100\n",
            "612/612 [==============================] - 0s 799us/step - loss: 1.9500 - acc: 0.1029 - mean_absolute_error: 0.2349 - val_loss: 2.1135 - val_acc: 0.2941 - val_mean_absolute_error: 0.2372\n",
            "Epoch 8/100\n",
            "612/612 [==============================] - 0s 806us/step - loss: 1.7475 - acc: 0.3382 - mean_absolute_error: 0.2269 - val_loss: 2.0927 - val_acc: 0.2092 - val_mean_absolute_error: 0.2329\n",
            "Epoch 9/100\n",
            "512/612 [========================>.....] - ETA: 0s - loss: 1.6806 - acc: 0.2480 - mean_absolute_error: 0.2218"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-160c2bcaace9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     model.fit(x_train, y_train, batch_size=128, epochs=100, \n\u001b[1;32m     55\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m               class_weight=class_weights)\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# evaluate the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1248\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1249\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1251\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "1Gu5bKZGZSil",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Answers to Questions Ex02\n",
        "\n",
        "#### What happens if you make the embeddings trainable? <br/>\n",
        "- I have seen a tendency for the model to overfit with larger number of epochs 50+ compared to when the embedding vector stays fixed.<br/>\n",
        "- It's possible that the model starts to rely too much on certain features => **assumption:** maybe the ones customized too much to the training data => when dropout regularization is used the result can be slightly improved and the training error doesn't converge as faster towards zero. <br/>\n",
        "- The model required in general less neurons per layer or at least this is what I was able to pick up from the results => models with less neurons but a trainable embeddings matrix performed better than their counterparts with a fixed embeddings matrix => this is only true up to a certain threshold of number of units (e.g., around 128 per layer).\n",
        "\n",
        "\n",
        "#### Does it work better than your first MLP? <br/>\n",
        "- The best result of the sklearn MLP: 59.6% accuracy and the best result of Keras: 58.3% accuracy are comparable. <br/> => For a more results please see the next section (Results Ex02).\n",
        "- The sklearn model results was achieved using RandomGridSearchCV.\n",
        "- The Keras model was a bit more of an experimentation => see at the bottom of this answer post (\"best Keras results\"). Keras can get a lot better results when one or more convolutional steps are integrated. \n",
        "\n",
        "\n",
        "#### What happens if you change the number of hidden layers? <br/>\n",
        "- The training error converges faster towards zero but the validation accuracy converges at the same speed. Sometimes using just one hidden layer provided good results.<br/>\n",
        "- I was unable to bring the rate of convergence of the training error and validation error to approx. the same speed. The training loss always decrease a lot faster and the validation accuracy got rarely above 60% accuracy.\n",
        "\n",
        "\n",
        "#### Do you notice anything when you change the number of units per layer? \n",
        "- **Too few units** => underfitting the model cannot pick up anything i.e. cannot learn any patterns, etc. => result is even worse when the embeddings are fixed. \n",
        "- **Too many units** => the model learns the aspects of the training set perfectly (training loss=0.0 & acc=1.0) => strong overfit towards the training set, the model starts to rely very much on certain things that it has trained (even stronger when embeddings are trianable)\n",
        "\n",
        "\n",
        "#### The best Keras results was achieved using: \n",
        "- optimizer: Adam() and activation function: relu()\n",
        "- embedding layer (\"not trainable\")\n",
        "- GlobalMaxPoolingLayer() instead of Flatten() => **Note:** Flatten had a tendency to produce \"Out of memory expections when used in Google Colab with large embedding dimension, number of words and max sequence length\". Flattten() only worked up to 200 embedding dimensions using at most 2000 words and sequence length. Additionally, the max pooling layer reduced the number of parameters compared to flatten() => **2nd assumption**: there might be too many parameters which are trainable and the results of the model is more of a random result. \n",
        "- 1 hidden layer: 128 units, tanh activation, 0.005 kernel l2 regularization\n",
        "- 1 ouptulayer using softmax\n",
        "\n",
        "For the three different Keras optimizers listed above the following network setups have been tested:\n",
        "```\n",
        "# create a NN using a pre-trained embedding layer\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "# flatten the embedding matrix to use it in a feed forward nn\n",
        "x = Flatten()(embedded_sequences)\n",
        "# x = GlobalMaxPooling1D() => used instead of Flatten() sometimes provided better results\n",
        "\n",
        "# hidden layers - different combinations of layers and dropouts\n",
        "# units: range of 10, 128, 256, 512\n",
        "# dropout: 0.1, 0.3, 0.5\n",
        "# l2 reg: different levels 0.001-0.01\n",
        "\n",
        "# example below\n",
        "x = Dense(128, activation='tanh', kernel_regularizer=l2(0.005))(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(128, activation='tanh', kernel_regularizer=l2(0.005))(x)\n",
        "\n",
        "# output layer with softmax activation\n",
        "preds = Dense(len(classes), activation='softmax')(x)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zdb2ZIt27v9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results Ex02\n",
        "\n",
        "#### Task 1: Using Sklearn MLP\n",
        "- Default Settings of TfidfVectorizer: ngram_range=(1, 1), max_df=1.0, max_features=None.<br/>\n",
        "best parameters: {'solver': 'adam', 'hidden_layer_sizes': (100, 100), 'alpha': 0.001, 'activation': 'relu'} <br/>\n",
        "**validation score: 0.5796344647519582, test score: 0.5968586387434555**\n",
        "\n",
        "- Settings of TfidfVectorizer: n_gram_range=(1,4), max_df=0.99, max_features=3000<br/>\n",
        "best parameters: {'solver': 'sgd', 'hidden_layer_sizes': (100,), 'alpha': 0.001, 'activation': 'tanh'}<br/>\n",
        "**validation score: 0.5039164490861618, test score: 0.5078534031413613**\n",
        "\n",
        "```\n",
        "labels: ['TED' 'TEx' 'TxD' 'Txx' 'xED' 'xEx' 'xxD']\n",
        "[[ 0  0  0  1  0  0  1]\n",
        " [ 0  0  1  0  0  1  0]\n",
        " [ 1  0  7  3  0  1  3]\n",
        " [ 3  5 26 59  1  8 13]\n",
        " [ 0  0  0  0  0  0  1]\n",
        " [ 1  1  1  5  2 16  7]\n",
        " [ 1  1  2  3  0  1 15]]\n",
        "```\n",
        "\n",
        "- Settings of TfidfVectorizer: ngram_range=(1,2), max_df=0.99, max_features=5000<br/>\n",
        "best parameters: {'solver': 'adam', 'hidden_layer_sizes': (100, 100, 100), 'alpha': 0.001, 'activation': 'tanh'}<br/>\n",
        "**validation score: 0.5691906005221932, test score: 0.581151832460733**\n",
        "\n",
        "```\n",
        "labels: ['TED' 'TEx' 'TxD' 'Txx' 'xED' 'xEx' 'xxD']\n",
        "[[ 0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  0]\n",
        " [ 3  5  8  7  0  2  9]\n",
        " [ 1  2 16 67  0  7  6]\n",
        " [ 0  0  0  0  0  0  0]\n",
        " [ 2  4  3  0  2 21  4]\n",
        " [ 0  0  4  2  0  1 15]]\n",
        "```\n",
        "\n",
        "#### Task 2: Using Keras\n",
        "- **Experiment 1: with no class weights or sampling** => only the most instances of the majority classes (with the most instances in the data set) are classified correctly. All others are classified incorrect => predicts everything to be of class 'Txx' <br/>\n",
        "Optimizer: **SGD**, Best Result: **0.453125**\n",
        " \n",
        "```\n",
        "labels: ['TED' 'TEx' 'TxD' 'Txx' 'xED' 'xEx' 'xxD']\n",
        "[[ 0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  0]\n",
        " [ 2  0  3  4  0  0  0]\n",
        " [ 3  1 28 58  2  7 29]\n",
        " [ 0  0  0  0  0  0  0]\n",
        " [ 0  5  2  4  6 23  5]\n",
        " [ 1  1  2  2  1  0  3]]\n",
        "```\n",
        "\n",
        " - **Experiment 2: Using the following class_weights**: {0: 4.0210084033613445, 1: 3.7976190476190474, 2: 0.9428571428571428, 3: 0.35883014623172105, 4: 5.696428571428571, 5: 0.7902559867877786, 6: 0.8336236933797909}. The class_weights can be thought of a multiplicative factor of how much each category will be up/down sampled.<br/> \n",
        " => **Conclusion:** The model is still very focussed on the majority class even though the result is slightly better\n",
        "\n",
        "\n",
        "Optimizer: **Adam**, Best Result: **0.5833333333333334**\n",
        "\n",
        "```\n",
        "labels: ['TED' 'TEx' 'TxD' 'Txx' 'xED' 'xEx' 'xxD']\n",
        "[[ 0  0  0  0  0  0  0]\n",
        " [ 0  0  0  1  0  0  0]\n",
        " [ 0  0  0  2  0  0  1]\n",
        " [ 3  1 16 75  1  5 13]\n",
        " [ 0  0  0  0  0  0  0]\n",
        " [ 3  1  7  4  4 22  3]\n",
        " [ 1  1  7  4  0  2 15]]\n",
        "```\n",
        "\n",
        "Optimizer: **Adagrad**, Best Result: **0.578125**\n",
        "\n",
        "```\n",
        "labels: ['TED' 'TEx' 'TxD' 'Txx' 'xED' 'xEx' 'xxD']\n",
        " [[ 0  0  0  0  0  0  0]\n",
        " [ 0  0  0  1  0  0  0]\n",
        " [ 0  0  0  4  0  0  2]\n",
        " [ 4  1 14 74  1  6 11]\n",
        " [ 0  0  0  0  0  0  0]\n",
        " [ 3  1  9  5  3 21  3]\n",
        " [ 0  1  7  2  1  2 16]]\n",
        "```"
      ]
    }
  ]
}