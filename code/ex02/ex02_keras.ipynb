{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPDfqEoJzqrb"
   },
   "source": [
    "# Deep Learning for Natural Language Processing: Exercise 02\n",
    "Moritz Eck (14-715-296)<br/>\n",
    "University of Zurich\n",
    "\n",
    "Please see the section right at the bottom of this notebook for the discussion of the results as well as the answers to the exercise questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNvqfpAIIRjB"
   },
   "source": [
    "### Mount Google Drive (Please do this step first => only needs to be done once!)\n",
    "\n",
    "This mounts the user's Google Drive directly.\n",
    "\n",
    "On my personal machine inside the Google Drive folder the input files are stored in the following folder:<br/> \n",
    "**~/Google Drive/Colab Notebooks/ex02/**\n",
    "\n",
    "Below I have defined the default filepath as **default_fp = 'drive/Colab Notebooks/ex02/'**.<br/>\n",
    "Please change the filepath to the location where you have the input file and the glove embeddings saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQdGNRs3H76k"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "\n",
    "import getpass\n",
    "\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_E4VnfSwO6P"
   },
   "source": [
    "**Mount Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9CGOGBLMIrR"
   },
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Omwg_12FItFI"
   },
   "source": [
    "### Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6RuPbebIudp"
   },
   "outputs": [],
   "source": [
    "!pip install pandas==0.23.4\n",
    "!pip install numpy \n",
    "!pip install scikit-learn==0.20\n",
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzB5plD1I8bw"
   },
   "source": [
    "### Check that the GPU is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "PT7pCqDHIvQF",
    "outputId": "3b5cf50e-df7f-4255-93d6-99d9dbb536d1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9AHoHfqQJi_B"
   },
   "source": [
    "### Preprocessing TED Talks File\n",
    "- Run Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "FrefaRtMJrUy",
    "outputId": "4dbbba79-381c-454d-c463-f8309e7f94af"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 200\n",
    "seed = np.random.seed(seed)\n",
    "\n",
    "# default inputs filepath\n",
    "default_fp = 'drive/Colab Notebooks/ex02/'\n",
    "ted_file = default_fp + './inputs/ted_en-20160408.xml'\n",
    "\n",
    "# constants\n",
    "lines = []\n",
    "talks = {}\n",
    "relevant = False\n",
    "talk = \"\"\n",
    "key = \"\"\n",
    "talk_count = 0\n",
    "\n",
    "print(\"Preprocessing TED Talks...\")\n",
    "\n",
    "with open(ted_file, 'r', encoding='utf-8') as freader:\n",
    "    lines = freader.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    # determine according to the keywords if the talk is relevant\n",
    "    if '<keywords' in line:\n",
    "        key = \"\"\n",
    "\n",
    "        if 'technology' in line:\n",
    "            key += \"T\"\n",
    "        else:\n",
    "            key += 'x'          \n",
    "\n",
    "        if 'entertainment' in line:\n",
    "            key += \"E\"\n",
    "        else:\n",
    "            key += 'x'  \n",
    "\n",
    "        if 'design' in line:\n",
    "            key += 'D'\n",
    "        else:\n",
    "            key += 'x'\n",
    "\n",
    "        if key != 'xxx':\n",
    "            relevant = True\n",
    "            continue\n",
    "        else:\n",
    "            relevant = False\n",
    "            continue\n",
    "\n",
    "    if not relevant:\n",
    "        continue\n",
    "    \n",
    "    # start reading the content\n",
    "    if '<transcription>' in line:\n",
    "        talk = \"\"\n",
    "\n",
    "    # append each line of the transcript\n",
    "    elif '<seekvideo' in line:\n",
    "        start = line.find('>') + 1\n",
    "        end = line.rfind('<') \n",
    "        talk += line[start:end] + \" \"\n",
    "\n",
    "    # end of a talk\n",
    "    elif '</transcription>' in line: \n",
    "        # store each talk with key and content\n",
    "        talks[talk_count] = [key, talk]\n",
    "        talk_count += 1\n",
    "        relevant = False\n",
    "\n",
    "# transform the dict into a dataframe\n",
    "df = pd.DataFrame.from_dict(talks, orient='index')\n",
    "df = df.rename(index=str, columns={0:'label', 1:'talk'})\n",
    "df = df.reset_index().drop(columns=['index'])\n",
    "print(df.describe())\n",
    "\n",
    "# link to pretrained embeddings tutorial\n",
    "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "# https://fasttext.cc/docs/en/english-vectors.html\n",
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_Bh7m0lTSO5"
   },
   "source": [
    "### Preprocessing Pre-Trained Word Embeddings\n",
    "- Run Once\n",
    "- If the embedding dimension shall be changed => change the filepath to the respective 60d/100d/200d/300d.Then re-run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mV8Hq1yNO_P2",
    "outputId": "ed893722-6058-47fd-c382-b100c89e6c68"
   },
   "outputs": [],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "glove_file = default_fp + './glove/glove.6B.100d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(glove_file, encoding='utf-8') as freader:\n",
    "    for line in freader:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-hktXgW7qfD"
   },
   "source": [
    "### Basic Sklearn MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eY6gvvR37rp0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# get the preprocessed data\n",
    "df = df.copy()\n",
    "\n",
    "# compute the class weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(df['label']), df['label'])\n",
    "\n",
    "for label, weight in zip(np.unique(df['label']), class_weights):\n",
    "    print(\"label: {} -> weight: {}\".format(label, weight))\n",
    "\n",
    "# split in training and test set\n",
    "train = df.sample(frac=0.8, random_state=seed)\n",
    "test = df.drop(train.index)\n",
    "\n",
    "# for training\n",
    "y_train = train['label']\n",
    "x_train = train.drop('label', axis=1)\n",
    "x_train = x_train['talk'].values\n",
    "\n",
    "# for testing\n",
    "y_test = test['label']\n",
    "x_test = test.drop('label', axis=1)\n",
    "x_test = x_test['talk'].values\n",
    "\n",
    "print('Training samples shape: ', x_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test samples shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "# encode the label\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "# transform the talks\n",
    "tfvect = TfidfVectorizer(ngram_range=(1,3), max_df=0.99, max_features=None)\n",
    "tfvect.fit(x_train)\n",
    "x_train = tfvect.transform(x_train)\n",
    "x_test = tfvect.transform(x_test)\n",
    "\n",
    "# standardize the data\n",
    "stand = StandardScaler(with_mean=False, with_std=True)\n",
    "stand.fit(x_train)\n",
    "x_train = stand.transform(x_train)\n",
    "x_test = stand.transform(x_test)\n",
    "\n",
    "# setup base mlp classifier\n",
    "mlp = MLPClassifier(early_stopping=True, validation_fraction=0.2, random_state=seed, batch_size='auto', \n",
    "                    max_iter=200, n_iter_no_change=15, learning_rate='adaptive', verbose=True)\n",
    "\n",
    "# set up parameter grid to evaluate over\n",
    "param_grid = dict(hidden_layer_sizes=[(100,100,100), (100,100), (100,)], \n",
    "                  solver = ['adam', 'sgd'], activation=['tanh', 'relu'], alpha=[0.001, 0.0001])\n",
    "\n",
    "# train mlp classifier using randomized grid search\n",
    "gs_mlp = RandomizedSearchCV(mlp, param_grid, n_iter=5, cv=5, n_jobs=4, verbose=True, refit=True)\n",
    "gs_mlp.fit(x_train, y_train)\n",
    "\n",
    "# print the best parameters of the evaluation\n",
    "print(gs_mlp.best_params_)\n",
    "print(gs_mlp.best_score_)\n",
    "\n",
    "# predict the test label\n",
    "y_pred = gs_mlp.predict(x_test)\n",
    "\n",
    "# print the accuracy and the confusion matrix\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0J6vzkXT-Nd"
   },
   "source": [
    "### Train Keras Model\n",
    "Run only once or once the model input parameters have been change: \n",
    "- e.g. changing input/embedding dimensions, MAX_NUM_WORDS, MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lvdle6a-T_rq"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# constants\n",
    "MAX_SEQUENCE_LENGTH = 2000\n",
    "MAX_NUM_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# input data\n",
    "texts = df['talk'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# compute the class weight\n",
    "classes = np.unique(df['label'])\n",
    "class_weights = compute_class_weight('balanced', classes, df['label'])\n",
    "class_weights = {label:weight for label, weight in zip(range(len(classes)), class_weights)}\n",
    "\n",
    "# encode the label as numerical value\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
    "                      lower=True, split=' ', char_level=False)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# one-hot encode the labels\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training, validation and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, stratify=labels, \n",
    "                                                    test_size=VALIDATION_SPLIT, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, stratify=y_train, \n",
    "                                                  test_size=VALIDATION_SPLIT, shuffle=True)\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YviO6PBFKuX"
   },
   "source": [
    "### Model Training\n",
    "- Change the parameters\n",
    "- Rerun with new configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4042
    },
    "colab_type": "code",
    "id": "b-j3YzGBAQlF",
    "outputId": "118c6c54-adbc-4325-8027-ab016f0bec5e"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# load pre-trained word embeddings into an embedding layer\n",
    "# if trainable = False, it will keep the embeddings fixed\n",
    "embedding_layer = Embedding(input_dim=num_words, output_dim=EMBEDDING_DIM, \n",
    "                            embeddings_initializer=Constant(embedding_matrix), \n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# create a NN using a pre-trained embedding layer\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# flatten the embedding matrix to use it in a feed forward nn\n",
    "x = Conv1D(filters=128, kernel_size=10, padding='same', activation='relu')(embedded_sequences)\n",
    "# x = Flatten()(embedded_sequences)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "# hidden layers\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(0.1))(x)\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(0.2))(x)\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(0.3))(x)\n",
    "\n",
    "# output layer with softmax activation\n",
    "preds = Dense(len(classes), activation='softmax')(x)\n",
    "\n",
    "# different optimizers\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) \n",
    "adagrad = Adagrad()\n",
    "adam = Adam()\n",
    "\n",
    "for opt in [adam, adagrad, sgd]:\n",
    "    # build, compile and print summary of the current model\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc', 'mae'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # stop training if paramter stops improving for x epochs\n",
    "    callbacks = [EarlyStopping(patience=25, monitor='val_acc')]\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=100, \n",
    "              validation_data=(x_val, y_val), callbacks=callbacks,\n",
    "              class_weight=class_weights)\n",
    "\n",
    "    # evaluate the model on the test set\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # convert one hot back to categorical\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test_ = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # evaluate the model using the test set\n",
    "    print(accuracy_score(y_pred, y_test_))\n",
    "    print(label_encoder.classes_)\n",
    "    print(class_weights)\n",
    "    print(confusion_matrix(y_pred, y_test_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Gu5bKZGZSil"
   },
   "source": [
    "## Answers to Questions Ex02\n",
    "\n",
    "#### What happens if you make the embeddings trainable? <br/>\n",
    "- I have seen a tendency for the model to overfit with larger number of epochs 50+ compared to when the embedding vector stays fixed.<br/>\n",
    "- It's possible that the model starts to rely too much on certain features => **assumption:** maybe the ones customized too much to the training data => when dropout regularization is used the result can be slightly improved and the training error doesn't converge as faster towards zero. <br/>\n",
    "- The model required in general less neurons per layer or at least this is what I was able to pick up from the results => models with less neurons but a trainable embeddings matrix performed better than their counterparts with a fixed embeddings matrix => this is only true up to a certain threshold of number of units (e.g., around 128 per layer).\n",
    "\n",
    "\n",
    "#### Does it work better than your first MLP? <br/>\n",
    "- The best result of the sklearn MLP: 59.6% accuracy and the best result of Keras: 58.3% accuracy are comparable. <br/> => For a more results please see the next section (Results Ex02).\n",
    "- The sklearn model results was achieved using RandomGridSearchCV.\n",
    "- The Keras model was a bit more of an experimentation => see at the bottom of this answer post (\"best Keras results\"). Keras can get a lot better results when one or more convolutional steps are integrated. \n",
    "\n",
    "\n",
    "#### What happens if you change the number of hidden layers? <br/>\n",
    "- The training error converges faster towards zero but the validation accuracy converges at the same speed. Sometimes using just one hidden layer provided good results.<br/>\n",
    "- I was unable to bring the rate of convergence of the training error and validation error to approx. the same speed. The training loss always decrease a lot faster and the validation accuracy got rarely above 60% accuracy.\n",
    "\n",
    "\n",
    "#### Do you notice anything when you change the number of units per layer? \n",
    "- **Too few units** => underfitting the model cannot pick up anything i.e. cannot learn any patterns, etc. => result is even worse when the embeddings are fixed. \n",
    "- **Too many units** => the model learns the aspects of the training set perfectly (training loss=0.0 & acc=1.0) => strong overfit towards the training set, the model starts to rely very much on certain things that it has trained (even stronger when embeddings are trianable)\n",
    "\n",
    "\n",
    "#### The best Keras results was achieved using: \n",
    "- optimizer: Adam() and activation function: relu()\n",
    "- embedding layer (\"not trainable\")\n",
    "- GlobalMaxPoolingLayer() instead of Flatten() => **Note:** Flatten had a tendency to produce \"Out of memory expections when used in Google Colab with large embedding dimension, number of words and max sequence length\". Flattten() only worked up to 200 embedding dimensions using at most 2000 words and sequence length. Additionally, the max pooling layer reduced the number of parameters compared to flatten() => **2nd assumption**: there might be too many parameters which are trainable and the results of the model is more of a random result. \n",
    "- 1 hidden layer: 128 units, tanh activation, 0.005 kernel l2 regularization\n",
    "- 1 ouptulayer using softmax\n",
    "\n",
    "For the three different Keras optimizers listed above the following network setups have been tested:\n",
    "```\n",
    "# create a NN using a pre-trained embedding layer\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# flatten the embedding matrix to use it in a feed forward nn\n",
    "x = Flatten()(embedded_sequences)\n",
    "# x = GlobalMaxPooling1D() => used instead of Flatten() sometimes provided better results\n",
    "\n",
    "# hidden layers - different combinations of layers and dropouts\n",
    "# units: range of 10, 128, 256, 512\n",
    "# dropout: 0.1, 0.3, 0.5\n",
    "# l2 reg: different levels 0.001-0.01\n",
    "\n",
    "# example below\n",
    "x = Dense(128, activation='tanh', kernel_regularizer=l2(0.005))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation='tanh', kernel_regularizer=l2(0.005))(x)\n",
    "\n",
    "# output layer with softmax activation\n",
    "preds = Dense(len(classes), activation='softmax')(x)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdb2ZIt27v9v"
   },
   "source": [
    "## Results Ex02\n",
    "\n",
    "#### Task 1: Using Sklearn MLP\n",
    "- Default Settings of TfidfVectorizer: ngram_range=(1, 1), max_df=1.0, max_features=None.<br/>\n",
    "best parameters: {'solver': 'adam', 'hidden_layer_sizes': (100, 100), 'alpha': 0.001, 'activation': 'relu'} <br/>\n",
    "**validation score: 0.5796344647519582, test score: 0.5968586387434555**\n",
    "\n",
    "- Settings of TfidfVectorizer: n_gram_range=(1,4), max_df=0.99, max_features=3000<br/>\n",
    "best parameters: {'solver': 'sgd', 'hidden_layer_sizes': (100,), 'alpha': 0.001, 'activation': 'tanh'}<br/>\n",
    "**validation score: 0.5039164490861618, test score: 0.5078534031413613**\n",
    "\n",
    "```\n",
    "labels: ['TED' 'TEx' 'TxD' 'Txx' 'xED' 'xEx' 'xxD']\n",
    "[[ 0  0  0  1  0  0  1]\n",
    " [ 0  0  1  0  0  1  0]\n",
    " [ 1  0  7  3  0  1  3]\n",
    " [ 3  5 26 59  1  8 13]\n",
    " [ 0  0  0  0  0  0  1]\n",
    " [ 1  1  1  5  2 16  7]\n",
    " [ 1  1  2  3  0  1 15]]\n",
    "```\n",
    "\n",
    "- Settings of TfidfVectorizer: ngram_range=(1,2), max_df=0.99, max_features=5000<br/>\n",
    "best parameters: {'solver': 'adam', 'hidden_layer_sizes': (100, 100, 100), 'alpha': 0.001, 'activation': 'tanh'}<br/>\n",
    "**validation score: 0.5691906005221932, test score: 0.581151832460733**\n",
    "\n",
    "```\n",
    "labels: ['TED' 'TEx' 'TxD' 'Txx' 'xED' 'xEx' 'xxD']\n",
    "[[ 0  0  0  0  0  0  0]\n",
    " [ 0  0  0  0  0  0  0]\n",
    " [ 3  5  8  7  0  2  9]\n",
    " [ 1  2 16 67  0  7  6]\n",
    " [ 0  0  0  0  0  0  0]\n",
    " [ 2  4  3  0  2 21  4]\n",
    " [ 0  0  4  2  0  1 15]]\n",
    "```\n",
    "\n",
    "#### Task 2: Using Keras\n",
    "- **Experiment 1: with no class weights or sampling** => only the most instances of the majority classes (with the most instances in the data set) are classified correctly. All others are classified incorrect => predicts everything to be of class 'Txx' <br/>\n",
    "Optimizer: **SGD**, Best Result: **0.453125**\n",
    " \n",
    "```\n",
    "labels: ['TED' 'TEx' 'TxD' 'Txx' 'xED' 'xEx' 'xxD']\n",
    "[[ 0  0  0  0  0  0  0]\n",
    " [ 0  0  0  0  0  0  0]\n",
    " [ 2  0  3  4  0  0  0]\n",
    " [ 3  1 28 58  2  7 29]\n",
    " [ 0  0  0  0  0  0  0]\n",
    " [ 0  5  2  4  6 23  5]\n",
    " [ 1  1  2  2  1  0  3]]\n",
    "```\n",
    "\n",
    " - **Experiment 2: Using the following class_weights**: {0: 4.0210084033613445, 1: 3.7976190476190474, 2: 0.9428571428571428, 3: 0.35883014623172105, 4: 5.696428571428571, 5: 0.7902559867877786, 6: 0.8336236933797909}. The class_weights can be thought of a multiplicative factor of how much each category will be up/down sampled.<br/> \n",
    " => **Conclusion:** The model is still very focussed on the majority class even though the result is slightly better\n",
    "\n",
    "\n",
    "Optimizer: **Adam**, Best Result: **0.5833333333333334**\n",
    "\n",
    "```\n",
    "labels: ['TED' 'TEx' 'TxD' 'Txx' 'xED' 'xEx' 'xxD']\n",
    "[[ 0  0  0  0  0  0  0]\n",
    " [ 0  0  0  1  0  0  0]\n",
    " [ 0  0  0  2  0  0  1]\n",
    " [ 3  1 16 75  1  5 13]\n",
    " [ 0  0  0  0  0  0  0]\n",
    " [ 3  1  7  4  4 22  3]\n",
    " [ 1  1  7  4  0  2 15]]\n",
    "```\n",
    "\n",
    "Optimizer: **Adagrad**, Best Result: **0.578125**\n",
    "\n",
    "```\n",
    "labels: ['TED' 'TEx' 'TxD' 'Txx' 'xED' 'xEx' 'xxD']\n",
    " [[ 0  0  0  0  0  0  0]\n",
    " [ 0  0  0  1  0  0  0]\n",
    " [ 0  0  0  4  0  0  2]\n",
    " [ 4  1 14 74  1  6 11]\n",
    " [ 0  0  0  0  0  0  0]\n",
    " [ 3  1  9  5  3 21  3]\n",
    " [ 0  1  7  2  1  2 16]]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ex02_Moritz_Eck.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
