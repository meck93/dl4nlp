Compare the output of the SVC and the SMC:
Model Settings: 
{lowercase=True, ngram_range=(1,2), max_df=0.99, min_df=1, max_features=500}
- only use the top 500 features (otherwise the algorithm takes way too long on the laptop)
- only unigrams and bigrams
- if a term occurs in 99% of all tweets than it is removed

Output Results for SVC
- accuracy: 0.4378698224852071

Output Result for SMC
- accuracy: 0.427810650887574

- Difference in performance is around 1%. I assume this is due to the difference (SMC - probability output and SVC - hard label assignment) but more below...

What additional information do we get from the SMC?
- The multi class hinge loss is a hard class assignment (no class membership probabilities only discrete labels). 
- The cross entropy loss measure the dissimilarity between the true label distribution and the predicted label distribution. Returns a vector with probabilities.
- The advantage with the SMC over the SVC is that the SMC assign a class membership probability (a vector of weights) as label to an instance (since it uses the softmax function). Even though the probaility is turned into a hard classification to assign a label to the instance during the computation the probability output of the SMC is used. On the other hand, the SVC performs a hard classification and assign a discrete label. 
