{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification - Language Classification\n",
    "\n",
    "This notebook implements the method presented in Goldberg's [2017] book \"Neural Network Methods for Natural Language Processing\". It shows the steps you need to go through in order to successfully train a classifier, and it should also, so I hope, illustrate the notational differences between Goldberg and standard machine learning literature.\n",
    "\n",
    "$NOTE$: There is no cross-validation etc. to find optimal parameters. This is simply to show how multi-class classification works. This will be part of a tutorial session and all other concepts will be explained there.\n",
    "\n",
    "Author: Phillip Ströbel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and cleaning the data\n",
    "\n",
    "The data consists of downloaded Wikipedia articles (see `urls.txt`) in German, English, French, Spanish, Italian and Finnish (instead of \"O\" in Goldberg). The data is in HTML, so we need to some preprocessing to get the text out of it. We also restrict ourselfes to the characters from a to z in the alphabet (as described in Goldberg). In this fashion, we get rid of all the Umlauts (ä, ö, ü) and all other characters with diacritics (as, e.g., the é or ç in French). Note however, that if these characters ocurring in bigrams would probably be good features. In some way, we still keep the information \"special character\" by not fully deleting the character, but by replacing it by the dollar sign \"\\$\". Furthermore, we replace all punctuation marks and digits by dollar signs as well. As such, all special characters, digits, and punctuation marks are mapped to $. The space will be replaced by an underscore \"\\_\". We then represent each langauge by 28 characters, as is suggested by Goldberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning HTML\n",
    "We first strip the HTML to get only the text of the Wikipedia page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "\n",
    "seed = np.random.seed(seed=200)  # set a seed for random, so results are reproducible\n",
    "\n",
    "article_dict = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "regex = r'[\\n ]{2,}'\n",
    "pattern = re.compile(regex)\n",
    "\n",
    "urls = open('urls.txt', 'r').readlines()\n",
    "\n",
    "for url_index, url in enumerate(urls):\n",
    "    language = url[8:10]\n",
    "    doc_id = 'doc_%d' % url_index\n",
    "    html = urlopen(url.strip()).read()    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    raw = soup.body.get_text()  # only get text from the text body (this excludes headers and should exclude navigation bars)\n",
    "    raw = re.sub(pattern, ' ', raw)  # replace multiple breaks and spaces by only one space\n",
    "    raw = re.sub(r'\\n', ' ', raw)  # replace every line break with a space\n",
    "    article_dict[language][doc_id] = raw.lower()  # assign each text to its language and lower all uppercase characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing --> prepare the text\n",
    "replace special characters and digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dict = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "abc = r'[a-z]'\n",
    "abc_pattern = re.compile(abc)\n",
    "\n",
    "for lang, doc in article_dict.items():\n",
    "    for doc, text in doc.items():\n",
    "        for char in text:\n",
    "            if re.match(abc_pattern, char):\n",
    "                preprocessed_dict[lang][doc] += char\n",
    "            elif re.match(' ', char):\n",
    "                preprocessed_dict[lang][doc] += '_'\n",
    "            else:\n",
    "                preprocessed_dict[lang][doc] += '$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count bigrams --> Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of bigrams will be our only feature. We could extend this by taking into account other n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = 'abcdefghijklmnopqrstuvwxyz$_'  # define the character set we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement, permutations\n",
    "\n",
    "def bigrams(text):\n",
    "    \"\"\"\n",
    "    Function to extract bigrams from text and calculate their distribution\n",
    "    :param text: text string\n",
    "    :return: dictionary containing bigrams as keys, and the normalised count as values\n",
    "    \"\"\"\n",
    "    combs = combinations_with_replacement(charset, 2)\n",
    "    perms = permutations(charset, 2)\n",
    "    bigram_dict = dict()\n",
    "    \n",
    "    for comb in set(list(combs) + list(perms)):\n",
    "        bigram_dict[''.join(comb)] = 0\n",
    "        \n",
    "    doc_length = len(text)\n",
    "    \n",
    "    for i in range(0, len(text)-1):\n",
    "        bigram = text[i] + text[i+1]\n",
    "        bigram_dict[bigram] += 1\n",
    "                \n",
    "    for bigram, count in bigram_dict.items():\n",
    "        bigram_dict[bigram] = count/doc_length\n",
    "\n",
    "    return bigram_dict              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put data into pandas dataframe\n",
    "The pandas dataframe allows us to conveniently represent all the data we need in one table. So let's do this. But first we need to extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict_full = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for lang, doc in preprocessed_dict.items():\n",
    "    for doc, text in sorted(doc.items()):\n",
    "        bigram_dict = bigrams(text)\n",
    "        bigram_dict_full[lang][doc] = bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$$</th>\n",
       "      <th>$_</th>\n",
       "      <th>$a</th>\n",
       "      <th>$b</th>\n",
       "      <th>$c</th>\n",
       "      <th>$d</th>\n",
       "      <th>$e</th>\n",
       "      <th>$f</th>\n",
       "      <th>$g</th>\n",
       "      <th>$h</th>\n",
       "      <th>...</th>\n",
       "      <th>zq</th>\n",
       "      <th>zr</th>\n",
       "      <th>zs</th>\n",
       "      <th>zt</th>\n",
       "      <th>zu</th>\n",
       "      <th>zv</th>\n",
       "      <th>zw</th>\n",
       "      <th>zx</th>\n",
       "      <th>zy</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.063981</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.071807</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.052874</td>\n",
       "      <td>0.028307</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.075421</td>\n",
       "      <td>0.035189</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062161</td>\n",
       "      <td>0.028908</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         $$        $_        $a        $b        $c        $d        $e  \\\n",
       "0  0.063981  0.030567  0.000434  0.000346  0.000499  0.000193  0.000346   \n",
       "1  0.071807  0.033520  0.000514  0.001359  0.000879  0.000597  0.000821   \n",
       "2  0.052874  0.028307  0.000814  0.000423  0.000462  0.000282  0.000454   \n",
       "3  0.075421  0.035189  0.000400  0.000466  0.000562  0.000268  0.000303   \n",
       "4  0.062161  0.028908  0.000513  0.000328  0.000520  0.000356  0.000629   \n",
       "\n",
       "         $f        $g        $h    ...           zq        zr        zs  \\\n",
       "0  0.000201  0.000177  0.000290    ...     0.000000  0.000000  0.000000   \n",
       "1  0.000390  0.000406  0.000373    ...     0.000000  0.000000  0.000025   \n",
       "2  0.000243  0.000172  0.000227    ...     0.000000  0.000000  0.000039   \n",
       "3  0.000224  0.000308  0.000189    ...     0.000000  0.000004  0.000000   \n",
       "4  0.000192  0.000185  0.000171    ...     0.000007  0.000007  0.000000   \n",
       "\n",
       "         zt        zu   zv        zw   zx        zy        zz  \n",
       "0  0.000000  0.000008  0.0  0.000000  0.0  0.000000  0.000016  \n",
       "1  0.000008  0.000050  0.0  0.000041  0.0  0.000000  0.000017  \n",
       "2  0.000000  0.000016  0.0  0.000000  0.0  0.000016  0.000008  \n",
       "3  0.000000  0.000013  0.0  0.000000  0.0  0.000009  0.000000  \n",
       "4  0.000000  0.000027  0.0  0.000000  0.0  0.000014  0.000021  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_names = ['y'] + sorted(bigram_dict_full['en']['doc_0'].keys())\n",
    "my_df = dict()\n",
    "\n",
    "for col in col_names:\n",
    "    my_df[col] = list()\n",
    "    \n",
    "df = pd.DataFrame(my_df)\n",
    "\n",
    "for lang, doc in bigram_dict_full.items():\n",
    "    for key, value in doc.items():\n",
    "        df_obj = value\n",
    "        df_obj['y'] = lang\n",
    "        df = df.append(df_obj, ignore_index=True)\n",
    "        \n",
    "df.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 785)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into the label vector \\begin{equation}\\mathbf{y}\\end{equation} and a training data matrix \\begin{equation}\\mathbf{X}\\end{equation}. But first, we shuffle the df and split it into a training and a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a train set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.sample(frac=0.9, random_state=seed)\n",
    "test = df.drop(train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training\n",
    "y_train = train.y\n",
    "X_train = train.drop('y', axis=1)\n",
    "\n",
    "# for testing\n",
    "y_test = test.y\n",
    "X_test = test.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples shape:  (54, 784)\n",
      "Training labels shape:  (54,)\n",
      "Test samples shape:  (6, 784)\n",
      "Test labels shape:  (6,)\n"
     ]
    }
   ],
   "source": [
    "print('Training samples shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test samples shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, it is necessary for many machine learning tasks to standardise the data. Our aim is for each feature to be represented by a column vector in which values have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_matrix(matrix, mean_and_std=None):\n",
    "    \"\"\"\n",
    "    normalises the data matrix (normalise each datapoint to zero mean and unit variance.)\n",
    "    :param matrix: input matrix\n",
    "    :param mean_and_std: provide mean and std as tuples in list for normalisation of test data\n",
    "    :return: normalised matrix and list consisting of tuples containing mean and std\n",
    "    \"\"\"\n",
    "    normalised = np.ones(matrix.shape)\n",
    "    \n",
    "    if mean_and_std == None:\n",
    "        \n",
    "        mean_std_list = list()\n",
    "\n",
    "        for col_index, col in enumerate(matrix):\n",
    "            mean = matrix[col].mean()\n",
    "            std = matrix[col].std()\n",
    "            mean_std_list.append((mean, std))\n",
    "            for row_index, item in enumerate(matrix[col]):\n",
    "                try:\n",
    "                    normalised[row_index, col_index] = (item - mean)/std\n",
    "                except ZeroDivisionError:\n",
    "                    normalised[row_index, col_index] = 0.0\n",
    "        return normalised, mean_std_list\n",
    "    else:\n",
    "        for col_index, col in enumerate(matrix):\n",
    "            for row_index, item in enumerate(matrix[col]):\n",
    "                try:\n",
    "                    mean = mean_and_std[col_index][0]\n",
    "                    std = mean_and_std[col_index][1]\n",
    "                    normalised[row_index, col_index] = (item - mean)/std\n",
    "                except ZeroDivisionError:\n",
    "                    normalised[row_index, col_index] = 0.0\n",
    "        return normalised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalise the train and test set (use values from training set for standardisation of test set). in sklearn, there are simple methods which help you with this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm, train_mean_std = normalise_matrix(X_train)\n",
    "test_norm = normalise_matrix(X_test, train_mean_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should binarise our labels, although libraries like sklearn can also deal with non-numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(['en', 'fr', 'de', 'it', 'es', 'fi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['de', 'en', 'es', 'fi', 'fr', 'it'], dtype='<U2')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this for both our training and test labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = lb.transform(y_train)\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are now one-hot encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We almost have everything now. However, we need to take care of the bias and the weight matrix. The hypothesis ŷ is given by:\n",
    "\\begin{equation}\n",
    "\\mathbf{\\hat{y}}=\\mathbf{x}\\cdot\\mathbf{W}+\\mathbf{b}\n",
    "\\end{equation}\n",
    "We can achieve this by appending 1 to each feature vector x, and the whole weight vector b to the weight matrix W. This is called the bias trick. Note that the dimensions of X_train change, and that the weight matrix W will have match the dimensions (same number of rows as X has columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 785)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_vector = np.ones([train_norm.shape[0], 1])\n",
    "X_train = np.append(train_norm, bias_vector, axis=1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise weight matrix with small weights\n",
    "\n",
    "np.random.seed(seed=200)\n",
    "\n",
    "W = np.random.randn(X_train.shape[1], len(lb.classes_)) * 0.0001\n",
    "#W = np.zeros([X.shape[1], len(lb.classes_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dimensions are right. The dot product of a specific row from X_train and the weight matrix W constitutes a forward pass and calculates the score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 6)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 785)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0008988 , -0.00167437,  0.00149301,  0.00149552,  0.0036825 ,\n",
       "         0.00016393]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5:6].dot(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the values for the highest score of the dot product is not the score of the true label. Our aim is to change this by implementing a support vector classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0036825])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5:6].dot(W).max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0016743673923002834"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5:6].dot(W)[0][y_train[5:6].argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: we follow kind of a naive implementation. The aim is to be able to understand what is going on!\n",
    "\n",
    "In order to quantify how good (or how bad) our weight matrix W can predict the data in our training set, we need to implement a loss function. Here we take a go at the hinge loss, which tries to predict the correct class with a margin of at least one to all other classes (or in this case, like presented in Goldberg, to the class which does not equal the true class, but which scores highest). In my understanding, this is a one-vs-one approach (true class vs. class with highest score (but doesn't equal the true class))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(x, y, W, index):\n",
    "    \"\"\"\n",
    "    Calculates the loss of a single data point by taking the prediction of the correct value and the the prediction of\n",
    "    the value of next highest score, following Crammer and Singer (2001)\n",
    "    :param x: sample point x as a vector\n",
    "    :param y: correct label y for x as a vector\n",
    "    :param W: weight matrix\n",
    "    :param index: column index of data matrix X\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    y_index = y[index].argmax()\n",
    "    y_value = x.dot(W)[y_index]\n",
    "    y_hat_max_value = np.delete(x.dot(W), y_index).max()\n",
    "    #for j in range(0, y.shape[1]):  # in case we wanted to classify against all other classes (one-vs-all) --> currently one-vs-one\n",
    "        #if j == y_index:\n",
    "            #continue\n",
    "    loss += max(0, 1 - (y_value - y_hat_max_value))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With matrix multiplication, we could get all the scores at once. In the following, however, we focus on an approach which takes sample by sample and calculates the loss and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = X_train.dot(W)  # simple matrix multiplication to get all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.36822338e-03, -5.38690529e-03, -4.39624523e-03,\n",
       "        -6.44164976e-03,  9.18749565e-04,  3.58689231e-03],\n",
       "       [-2.99758798e-03,  1.00016692e-03,  3.83380346e-05,\n",
       "        -8.53791583e-04,  3.83284607e-03,  1.52095946e-04],\n",
       "       [-1.60844274e-04,  5.80906353e-03,  2.94484507e-03,\n",
       "         1.73589374e-03,  6.13581561e-04,  3.34736463e-03],\n",
       "       [-9.93770414e-04, -1.85561820e-04, -5.63257159e-04,\n",
       "         4.29132292e-03, -1.78035786e-03, -2.21281342e-03],\n",
       "       [-1.95033302e-03,  7.82514989e-05,  1.51759145e-03,\n",
       "        -8.38414430e-04, -1.77793019e-03, -4.54139891e-03],\n",
       "       [-8.98801826e-04, -1.67436739e-03,  1.49301055e-03,\n",
       "         1.49552287e-03,  3.68249626e-03,  1.63930726e-04],\n",
       "       [ 3.93043664e-04,  2.60245967e-03,  7.27111868e-04,\n",
       "         5.23099807e-03, -1.42932688e-03, -1.65149922e-03],\n",
       "       [ 9.09820602e-04,  1.28445802e-03,  6.74879950e-04,\n",
       "         2.00379697e-03,  7.29421319e-04, -3.86434529e-03],\n",
       "       [-5.95409881e-03, -1.29857122e-03,  1.37284914e-03,\n",
       "         4.41700227e-03, -2.24799299e-03, -3.26903608e-03],\n",
       "       [-8.74648852e-04,  4.28575083e-03,  7.93003524e-04,\n",
       "         1.07920654e-03,  1.64174053e-03, -4.80949399e-03],\n",
       "       [ 2.44139077e-03,  2.56490356e-03,  1.87941059e-03,\n",
       "         2.05889120e-04, -1.00061578e-04, -2.07492199e-03],\n",
       "       [ 3.93824371e-04,  1.65998775e-03,  2.55515761e-03,\n",
       "         4.10269178e-04,  9.52850677e-04,  9.52788836e-04],\n",
       "       [ 5.35862301e-04,  3.51538008e-03,  1.91943132e-03,\n",
       "        -9.98373285e-04, -1.89621710e-04,  1.54757468e-03],\n",
       "       [ 6.85248915e-04,  3.56290893e-03,  3.66697740e-03,\n",
       "        -4.68917956e-03,  7.34570495e-04,  5.74717625e-04],\n",
       "       [ 4.63527994e-03,  3.10155466e-03, -4.44663045e-05,\n",
       "         8.56613603e-04,  3.96315892e-04, -3.08052553e-03],\n",
       "       [ 2.55317192e-03,  2.38924335e-03,  1.63251780e-03,\n",
       "         3.27297980e-03,  6.98982206e-04,  1.69652010e-03],\n",
       "       [ 4.28963993e-04,  3.30687294e-03,  8.25475842e-04,\n",
       "         1.13649976e-04, -2.19547524e-03, -6.75179759e-04],\n",
       "       [-5.94033109e-04, -6.97364945e-04, -3.03770466e-03,\n",
       "         3.67213894e-04, -2.64694598e-03, -4.04927806e-03],\n",
       "       [-2.71277641e-03, -2.90921486e-03, -1.58369058e-03,\n",
       "        -5.01799012e-03, -2.04207136e-03,  2.24811414e-03],\n",
       "       [ 7.11136279e-03, -6.40956682e-04,  4.46441845e-03,\n",
       "         2.46115682e-03,  7.20167089e-04,  5.57167159e-03],\n",
       "       [ 3.32125833e-03,  1.76044975e-03, -1.92284569e-03,\n",
       "        -2.16139967e-04, -8.77965354e-04,  3.75112729e-03],\n",
       "       [-3.02773506e-03,  5.16950474e-04,  3.77681054e-06,\n",
       "        -1.60119456e-03,  6.74496817e-04, -2.63450087e-04],\n",
       "       [ 3.74147672e-03, -1.54290718e-03,  4.28787789e-03,\n",
       "        -5.01202798e-06,  3.31191657e-04,  1.53571704e-03],\n",
       "       [ 6.84770883e-04, -8.60532242e-04,  6.00818198e-03,\n",
       "         2.82566470e-03, -2.35251743e-03,  2.61611345e-04],\n",
       "       [-4.31973690e-03, -4.50107537e-03, -6.99644214e-03,\n",
       "        -5.61359745e-03,  1.77154206e-03,  3.99915344e-03],\n",
       "       [ 6.01078996e-03,  1.46882614e-03,  1.01656061e-03,\n",
       "         3.91181877e-03, -3.33946096e-04,  7.97476316e-04],\n",
       "       [ 4.34352386e-04,  3.18481344e-03,  5.46823584e-03,\n",
       "         2.10747238e-03,  1.33822892e-03, -4.17702405e-03],\n",
       "       [-9.60904707e-05, -2.97760781e-03,  2.45781315e-03,\n",
       "         1.82004058e-03,  1.22798618e-03,  1.57084107e-03],\n",
       "       [ 8.32566447e-04, -2.39582496e-03,  1.72306346e-03,\n",
       "         3.33865385e-03,  1.55145949e-03,  4.26390240e-03],\n",
       "       [-3.10417867e-03,  5.26605065e-04,  1.74685823e-03,\n",
       "        -3.49331250e-03, -2.65348247e-04,  1.81187081e-03],\n",
       "       [-1.10100879e-03,  4.68440883e-03,  2.45689358e-03,\n",
       "         5.47820970e-03, -1.38926002e-03,  4.48080305e-03],\n",
       "       [-2.86633137e-03,  8.69803045e-04, -2.13883749e-03,\n",
       "        -1.86635202e-03, -5.35277358e-05,  2.51506497e-03],\n",
       "       [-4.29467827e-04, -7.57687600e-04, -1.18839189e-04,\n",
       "         4.94515977e-03, -3.66739039e-03, -3.58876118e-03],\n",
       "       [-7.32636228e-04,  2.97131214e-03, -1.86507149e-04,\n",
       "        -2.39401067e-03,  2.20044515e-03, -3.41664009e-03],\n",
       "       [ 2.24303273e-03, -2.60276787e-03, -3.84265332e-03,\n",
       "        -6.09038545e-03,  1.02316314e-03, -3.32582424e-03],\n",
       "       [ 1.39205556e-03, -1.08263725e-03,  1.36532940e-03,\n",
       "         4.22399311e-03, -1.89448963e-04,  1.80878262e-04],\n",
       "       [-1.36209685e-03, -3.21194620e-03, -6.88967114e-04,\n",
       "         4.73163171e-03, -8.84039384e-04, -4.15357502e-03],\n",
       "       [ 4.29102808e-03, -3.37190416e-03, -2.27862654e-03,\n",
       "        -2.56242986e-03, -4.22751784e-03,  3.65908575e-03],\n",
       "       [-2.12692894e-03, -4.54883143e-03, -3.27737271e-03,\n",
       "        -3.94118270e-03, -1.88170522e-03,  2.06873816e-03],\n",
       "       [ 4.66996780e-03, -2.50058135e-03,  2.31065945e-03,\n",
       "        -3.62238095e-03, -2.33475264e-03,  4.86711120e-03],\n",
       "       [-1.75615603e-03, -1.01373772e-03, -1.13861171e-03,\n",
       "        -1.08722546e-03, -2.53484294e-03, -2.11661124e-03],\n",
       "       [ 1.76472853e-03, -9.30032264e-04,  1.76841186e-04,\n",
       "        -4.78929503e-04,  1.35800617e-05, -3.21854168e-03],\n",
       "       [-1.48577422e-03,  2.49804908e-03, -2.06508589e-03,\n",
       "        -3.81833380e-03,  3.08983131e-04,  3.16292282e-03],\n",
       "       [-1.16263880e-03,  3.68755970e-03,  5.54773067e-04,\n",
       "         4.14107770e-03,  9.52459677e-04, -1.28122657e-03],\n",
       "       [-3.57106118e-04,  1.45792598e-03,  3.20987850e-03,\n",
       "        -4.89923305e-04, -4.38384785e-04,  8.76703051e-05],\n",
       "       [ 4.30188145e-03, -3.44588396e-03, -2.47791126e-03,\n",
       "        -1.02778882e-03, -2.94377834e-03,  1.64917411e-03],\n",
       "       [-1.29745988e-03,  7.41531604e-04, -1.40382503e-03,\n",
       "         8.43310352e-04,  4.21048600e-04, -2.40418357e-03],\n",
       "       [-5.77631087e-03, -6.78989586e-03,  8.77687707e-04,\n",
       "        -5.67463092e-04,  8.20257179e-04, -7.86256569e-04],\n",
       "       [-4.65806402e-03, -9.35331939e-04, -4.12259568e-03,\n",
       "        -2.70770655e-03,  8.72366378e-04, -9.67181678e-04],\n",
       "       [ 1.72017314e-03,  1.69858614e-03,  9.34470766e-04,\n",
       "        -2.60453762e-03,  4.17926356e-03, -3.22291849e-03],\n",
       "       [ 6.11563303e-04, -5.08112228e-03, -3.08907407e-03,\n",
       "         3.74874667e-03,  1.33913564e-03, -5.38145040e-04],\n",
       "       [-1.84474113e-03, -1.05972580e-03, -4.25032654e-03,\n",
       "        -6.77409987e-03,  5.63949990e-03,  2.57651544e-03],\n",
       "       [ 6.30149386e-03,  3.73471594e-03, -6.43878337e-04,\n",
       "         4.25455047e-03, -2.28412882e-03,  3.44720487e-03],\n",
       "       [-1.78354958e-03, -5.53843887e-03, -5.61042919e-03,\n",
       "        -6.11862926e-03, -1.37791239e-03,  9.99055841e-04]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, W):\n",
    "    \"\"\"\n",
    "    compute the gradient\n",
    "    :param X: data matrix (train) \n",
    "    :param y: the corresponding \n",
    "    :param W: weight matrix\n",
    "    :return: loss and Jacobian dW with all gradients\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for index, x in enumerate(X):\n",
    "        y_index = y[index].argmax()\n",
    "        y_value = x.dot(W)[y_index]\n",
    "        y_hat_max_value = np.delete(x.dot(W), y_index).max()\n",
    "        loss = max(0, 1 - (y_value - y_hat_max_value))\n",
    "        total_loss += loss\n",
    "        y_hat_max_index = np.delete(x.dot(W), y_index).argmax() + 1\n",
    "        if loss > 0:  # not sure whether we need this if statement\n",
    "            dW[:, y_hat_max_index] += x.transpose()\n",
    "            dW[:, y_index] -= x.transpose()\n",
    "            \n",
    "    return total_loss, dW\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, W, eta, steps):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for a number of times with a fixed learning rate eta\n",
    "    :param X: data matrix\n",
    "    :param y: labels\n",
    "    :param W: weight matrix\n",
    "    :param eta: learning rate\n",
    "    :param steps: number of times gradient descent should be performed\n",
    "    :return: learned representation matrix W_learned\n",
    "    \"\"\"\n",
    "    W_learned = W.copy()\n",
    "    \n",
    "    for step in range(0, steps):\n",
    "        loss, dW = gradient(X, y, W_learned)\n",
    "        print(loss)\n",
    "        W_learned = W_learned - eta * dW\n",
    "        \n",
    "    return W_learned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.20764584833195\n",
      "17.178159125332577\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "W_star = gradient_descent(X_train, y_train, W, eta=0.001, steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Let's test if our learned representation of the data is any good at classifying the data in the test set. Of course we need the bias in our test set as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_vector_test = np.ones([X_test.shape[0], 1])\n",
    "X_test = np.append(test_norm, bias_vector_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "3 3\n",
      "3 3\n",
      "0 0\n",
      "4 4\n",
      "4 4\n"
     ]
    }
   ],
   "source": [
    "for index, x in enumerate(X_test.dot(W_star)):\n",
    "    pred = x.argmax()\n",
    "    true_label = y_test[index].argmax()\n",
    "    print(pred, true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad! But Goldberg mentioned something about regularisation, so we should take this into account as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_reg(X, y, W, lam):\n",
    "    \"\"\"\n",
    "    compute the gradient\n",
    "    :param X: data matrix (train) \n",
    "    :param y: the corresponding \n",
    "    :param W: weight matrix\n",
    "    :param lam: reguliser lambda\n",
    "    :return: Jacobian dW with all gradients\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for index, x in enumerate(X):\n",
    "        y_index = y[index].argmax()\n",
    "        y_value = x.dot(W)[y_index]\n",
    "        y_hat_max_value = np.delete(x.dot(W), y_index).max()\n",
    "        loss = max(0, 1 - (y_value - y_hat_max_value)) + lam * np.linalg.norm(W, 2)\n",
    "        total_loss += loss\n",
    "        y_hat_max_index = np.delete(x.dot(W), y_index).argmax() + 1\n",
    "        if loss > 0:  # not sure whether we need this if statement\n",
    "            dW[:, y_hat_max_index] += x.transpose()\n",
    "            dW[:, y_index] -= x.transpose()\n",
    "        \n",
    "    dW += lam * W\n",
    "            \n",
    "    return total_loss, dW\n",
    "\n",
    "def gradient_descent_reg(X, y, W, eta, steps):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for a number of times with a fixed learning rate eta\n",
    "    :param X: data matrix\n",
    "    :param y: labels\n",
    "    :param W: weight matrix\n",
    "    :param eta: learning rate\n",
    "    :param steps: number of times gradient descent should be performed\n",
    "    :return: learned representation matrix W_learned\n",
    "    \"\"\"\n",
    "    W_learned = W.copy()\n",
    "    \n",
    "    for step in range(0, steps):\n",
    "        loss, dW = gradient_reg(X, y, W_learned, -2)\n",
    "        print(loss)\n",
    "        W_learned = W_learned - eta * dW\n",
    "        \n",
    "    return W_learned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.8924078298456\n",
      "-19.915693715527116\n",
      "-34.72686132199791\n",
      "-34.796315044641915\n",
      "-34.86590767473121\n",
      "-34.93563949008068\n",
      "-35.005510769060876\n",
      "-35.0755217905989\n",
      "-35.145672834180196\n",
      "-35.21596417984851\n"
     ]
    }
   ],
   "source": [
    "W_star_reg = gradient_descent_reg(X_train, y_train, W, eta=0.001, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "3 3\n",
      "3 3\n",
      "0 0\n",
      "4 4\n",
      "4 4\n"
     ]
    }
   ],
   "source": [
    "for index, x in enumerate(X_test.dot(W_star_reg)):\n",
    "    pred = x.argmax()\n",
    "    true_label = y_test[index].argmax()\n",
    "    print(pred, true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the two different weight matrices (one regularised, the other not), we notice the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00204807,  0.00598506, -0.00640592,  0.0087149 , -0.00527147,\n",
       "        -0.00084924],\n",
       "       [-0.00441286,  0.00444758, -0.01524402, -0.00293755,  0.01245237,\n",
       "         0.00586772],\n",
       "       [-0.00745913, -0.00624353,  0.01561007, -0.00880273,  0.01712681,\n",
       "        -0.00989171],\n",
       "       [ 0.01586253, -0.00306909, -0.00304522, -0.00704329,  0.00327248,\n",
       "        -0.00608096],\n",
       "       [-0.00093228, -0.00380838,  0.0025021 , -0.00890512,  0.02599701,\n",
       "        -0.01478257]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_star[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00208552,  0.00609405, -0.00695889,  0.00887298, -0.0052297 ,\n",
       "        -0.00056512],\n",
       "       [-0.00449297,  0.00452838, -0.01590587, -0.00299069,  0.00967119,\n",
       "         0.00936672],\n",
       "       [-0.00759443, -0.00635661,  0.01505479, -0.00896256,  0.01282986,\n",
       "        -0.00462442],\n",
       "       [ 0.01615017, -0.00312473, -0.00260994, -0.00717084,  0.00172759,\n",
       "        -0.00507789],\n",
       "       [-0.00094926, -0.00387774,  0.00249645, -0.00906628,  0.01961233,\n",
       "        -0.00814331]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_star_reg[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By the way ...\n",
    "### In scikit-learn it's much easier to implement this :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=0,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0, multi_class='crammer_singer', loss='hinge')\n",
    "clf.fit(X_train, train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en', 'fi', 'fi', 'de', 'fr', 'fr'], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     en\n",
       "12    fi\n",
       "16    fi\n",
       "26    de\n",
       "41    fr\n",
       "42    fr\n",
       "Name: y, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with our naive implementation, we do not much worse than with scikit's. scikit's implementation is of course much more elaborate and uses the vectorised operation and possibly other optimisation techniques in order to make its SVM (or SVC) better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
