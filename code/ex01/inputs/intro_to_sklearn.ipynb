{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to introduce you to scikit-learn with the help of an instructional example about text classification. We will cover the most basic principles and ideas about scikit-learn in this notebook. This tutorial is inspired by the sklearn tutorial on http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html, but contains a few more explanations and is suited to introduce scikit-learn in class.\n",
    "\n",
    "$Author$: Phillip Ströbel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Get the data from http://qwone.com/~jason/20Newsgroups/. We will work with the 20news-bydate.tar.gz data set. Unzip it to a suitable destination. Here, all the data lies in the data folder. To our convenience, it has already been split into a training and a test set, so we don't have to care about this. What we need to do though is get the data and put it into a dataframe (you could also only work with dictionaries or other data containers). We do this for both the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "def create_df(path_to_data, categories=[], shuffle=True, random_state=42):\n",
    "    \"\"\"\n",
    "    takes the path of a folder containing all the subfolders (which contain the acutal documents). builds a pandas datafram with document ids, the text and the label. shuffled is default.\n",
    "    :param path_to_data: path to top folder as a string\n",
    "    :param categories: categories which should be in the dataframe as list of strings, default is all categories\n",
    "    :param shuffle: boolean, determines whether data should be shuffled or not\n",
    "    :param random_state: integer, seed for shuffling\n",
    "    :return: pandas dataframe with all th\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_list = list()\n",
    "    \n",
    "    if categories == []:\n",
    "        for category in os.listdir(path_to_data):\n",
    "            for document in os.listdir(os.path.join(path_to_data, category)):\n",
    "                doc = codecs.open(os.path.join(path_to_data, category, document), 'r', 'latin-1').read().replace('\\n', ' ')\n",
    "                doc_list.append([doc, category])\n",
    "    else:\n",
    "        for category in categories:\n",
    "            for document in os.listdir(os.path.join(path_to_data, category)):\n",
    "                doc = codecs.open(os.path.join(path_to_data, category, document), 'r', 'latin-1').read().replace('\\n', ' ')\n",
    "                doc_list.append([doc, category])\n",
    "    \n",
    "    df = pd.DataFrame(doc_list, columns=['text', 'label'])\n",
    "\n",
    "    return df.sample(frac=1, random_state=random_state)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_df('data/20news-bydate/20news-bydate-train')\n",
    "test = create_df('data/20news-bydate/20news-bydate-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size:  (11314, 2)\n",
      "test size:  (7532, 2)\n"
     ]
    }
   ],
   "source": [
    "print('training size: ', train.shape)\n",
    "print('test size: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11314</td>\n",
       "      <td>11314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>11314</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>From: mad9a@fermi.clas.Virginia.EDU (Michael A...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text             label\n",
       "count                                               11314             11314\n",
       "unique                                              11314                20\n",
       "top     From: mad9a@fermi.clas.Virginia.EDU (Michael A...  rec.sport.hockey\n",
       "freq                                                    1               600"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "alt.atheism                 480\n",
       "comp.graphics               584\n",
       "comp.os.ms-windows.misc     591\n",
       "comp.sys.ibm.pc.hardware    590\n",
       "comp.sys.mac.hardware       578\n",
       "comp.windows.x              593\n",
       "misc.forsale                585\n",
       "rec.autos                   594\n",
       "rec.motorcycles             598\n",
       "rec.sport.baseball          597\n",
       "rec.sport.hockey            600\n",
       "sci.crypt                   595\n",
       "sci.electronics             591\n",
       "sci.med                     594\n",
       "sci.space                   593\n",
       "soc.religion.christian      599\n",
       "talk.politics.guns          546\n",
       "talk.politics.mideast       564\n",
       "talk.politics.misc          465\n",
       "talk.religion.misc          377\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we split the labels from the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.text\n",
    "y_train = train.label\n",
    "X_test = test.text\n",
    "y_test = test.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we got this right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:  (11314,)\n",
      "Training labels shape:  (11314,)\n",
      "Test set shape:  (7532,)\n",
      "Test labels shape:  (7532,)\n"
     ]
    }
   ],
   "source": [
    "print('Training set shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test set shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 7492     From: d88-jwa@hemul.nada.kth.se (Jon Wätte) Su...\n",
       "3546     From: nyeda@cnsvax.uwec.edu (David Nye) Subjec...\n",
       "5582     From: rolfe@junior.dsu.edu (Tim Rolfe) Subject...\n",
       "4793     From: GMILLS@CHEMICAL.watstar.uwaterloo.ca (Ph...\n",
       "3813     From: km@cs.pitt.edu (Ken Mitchum) Subject: Re...\n",
       "4219     From: PA146008@utkvm1.utk.edu (David Veal) Sub...\n",
       "5211     From: belville@athena.mit.edu (Sharon Belville...\n",
       "1608     From: robert@cpuserver.acsc.com (Robert Grant)...\n",
       "8001     Subject: Re: PHILS, NL EAST NOT SO WEAK From: ...\n",
       "11128    From: brain@cbnewsj.cb.att.com (harish.s.mangr...\n",
       "3886     Subject: STARGARDTS DISEASE From: kmcvay@oneb....\n",
       "9658     From: hilmi-er@dsv.su.se (Hilmi Eren) Subject:...\n",
       "3400     From: bhjelle@carina.unm.edu () Subject: Re: M...\n",
       "2576     From: bskendig@netcom.com (Brian Kendig) Subje...\n",
       "4899     From: skucera@prstorm.bison.mb.ca (stan kucera...\n",
       "7434     From: bauer@informatik.uni-ulm.de (Christian B...\n",
       "2820     From: hesh@cup.hp.com (Chris Steinbroner) Subj...\n",
       "1718     From: rob@rjck.UUCP (Robert J.C. Kyanko) Subje...\n",
       "2189     From: michal+@cs.cmu.edu (Michal Prussak) Subj...\n",
       "1160     From: sundar@fiber-one.ai.mit.edu (Sundar Nara...\n",
       "6775     From: viola@asterix.uni-muenster.de (Jrg Viol...\n",
       "107      From: henry@zoo.toronto.edu (Henry Spencer) Su...\n",
       "3187     From: James Leo Belliveau <jbc9+@andrew.cmu.ed...\n",
       "2855     From: viking@iastate.edu (Dan Sorenson) Subjec...\n",
       "5516     From: gchin@ssf.Eng.Sun.COM (Gary Chin) Subjec...\n",
       "6029     From: J056600@LMSC5.IS.LMSC.LOCKHEED.COM Subje...\n",
       "4301     From: Clinton-HQ@Campaign92.Org (Clinton/Gore ...\n",
       "4339     From: cramer@optilink.COM (Clayton Cramer) Sub...\n",
       "2348     From: sprec-j@acsu.buffalo.edu (Joel Sprechman...\n",
       "8284     From: steph@cs.uiuc.edu (Dale Stephenson) Subj...\n",
       "                               ...                        \n",
       "2047     From: gkirkaldie@sanity.tdkcs.waterloo.on.ca (...\n",
       "7849     From: nodine@lcs.mit.edu (Mark H. Nodine) Subj...\n",
       "2558     From: brian@lpl.arizona.edu (Brian Ceccarelli ...\n",
       "9274     From: ptg2351@uxa.cso.uiuc.edu (Panos Tamamidi...\n",
       "8666     From: chiu@io.nosc.mil (Francis Chiu) Subject:...\n",
       "11096    From: dwilson@csugrad.cs.vt.edu (David Wilson)...\n",
       "6396     From: drisko@ics.com (Jason Drisko) Subject: R...\n",
       "3385     From: mhollowa@ic.sunysb.edu (Michael Holloway...\n",
       "4555     From: David A. Fuess Subject: Re: Windows 3.1 ...\n",
       "1184     From: pest@konech.UUCP (Wolfgang Pest) Subject...\n",
       "6420     Subject: Re: Trouble compiling X11R5 on SunOS_...\n",
       "5051     From: caralv@caralv.auto-trol.com (Carol Alvin...\n",
       "5311     From: rayssd!esther@uunet.uu.net (Esther A. Pa...\n",
       "2433     From: mcelwre@cnsvax.uwec.edu Subject: THE DIV...\n",
       "6949     From: smhanaes@gpu.utcc.utoronto.ca (D. Wiggle...\n",
       "10583    Subject: Re: Biblical Rape From: I3150101@dbst...\n",
       "769      From: sburton@dres.dnd.ca (Stan Burton) Subjec...\n",
       "1685     From: eapu207@orion.oac.uci.edu (John Peter Ko...\n",
       "8322     From: dkeisen@leland.Stanford.EDU (Dave Eisen)...\n",
       "11111    From: jamal@socrates.umd.edu (Jamal Asi) Subje...\n",
       "5578     From:  (Phil Bowermaster) Subject: C. S. Lewis...\n",
       "4426     From: mb4008@cehp11 (Morgan J Bullard) Subject...\n",
       "466      From: games@max.u.washington.edu Subject: SSTO...\n",
       "6265     From: gerard@dps.co.UK (Gerard O'Driscoll) Sub...\n",
       "5734     From: gld@cunixb.cc.columbia.edu (Gary L Dare)...\n",
       "11284    From: dwilson@csugrad.cs.vt.edu (David Wilson)...\n",
       "5191     From: mcovingt@aisun3.ai.uga.edu (Michael Covi...\n",
       "5390     From: jodfishe@silver.ucs.indiana.edu (joseph ...\n",
       "860      From: v064mb9k@ubvmsb.cc.buffalo.edu (NEIL B. ...\n",
       "7270     From: rlglende@netcom.com (Robert Lewis Glende...\n",
       "Name: text, Length: 11314, dtype: object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "So far, so good! But we know that machine learning algorithms cannot work with text data directly. So we need to vectorise the data somehow. also, we might do some preprocessing. Let's see how we can tackle these problems.\n",
    "### Vectorise the data \n",
    "Luckily, sklearn offers some nice classes which help us. We should tokenise the data and then vectorise it. Conveniently, sklearns `CountVectoriser()` does exactly that. Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the three central methods in sklearn are `transform`, `fit`, `fit_transform`, and `predict`. We will see how each of these work and when to use them. We have alredy made use of `fit_transform`. Instead of using this method, we could have called the method `fit` on the training set first and the use `transform` to vectorise the data (to 'transform' it). With the fitted `CountVectorizer` we can now transform other data, like for example the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will return to this later. First let us see what `CountVectorizer` produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorised form contains 11314 rows, which is the number of our documents, while the number of columns tells us something about the vocabulary size of the whole corpus. But what's a sparse matrix? Note that saving the complete, sparse document-vocabulary matrix would need to hold 1,472,030,598 values, most of which would be zero? Why? Instead, we only save 1,787,565 values in a compressed sparse row format. An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "mtx = sparse.csr_matrix((data, (row, col)), shape=(3, 3))\n",
    "mtx      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 2],\n",
       "        [0, 0, 3],\n",
       "        [4, 5, 6]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtx.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 127 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can which positions of the document vector are occupied. A `1` means the word occurs once in the document, while any other number gives the exact count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 51052)\t1\n",
      "  (0, 100156)\t1\n",
      "  (0, 2337)\t1\n",
      "  (0, 95956)\t1\n",
      "  (0, 29573)\t1\n",
      "  (0, 114731)\t1\n",
      "  (0, 46170)\t1\n",
      "  (0, 61538)\t1\n",
      "  (0, 78586)\t1\n",
      "  (0, 116834)\t1\n",
      "  (0, 39203)\t1\n",
      "  (0, 123759)\t1\n",
      "  (0, 29620)\t1\n",
      "  (0, 76551)\t1\n",
      "  (0, 95064)\t1\n",
      "  (0, 106209)\t1\n",
      "  (0, 59666)\t1\n",
      "  (0, 104494)\t1\n",
      "  (0, 95130)\t1\n",
      "  (0, 93699)\t1\n",
      "  (0, 101005)\t1\n",
      "  (0, 41525)\t1\n",
      "  (0, 114806)\t1\n",
      "  (0, 83858)\t1\n",
      "  (0, 86864)\t1\n",
      "  :\t:\n",
      "  (0, 110775)\t1\n",
      "  (0, 113812)\t1\n",
      "  (0, 89362)\t5\n",
      "  (0, 67602)\t1\n",
      "  (0, 103113)\t1\n",
      "  (0, 90379)\t1\n",
      "  (0, 64095)\t1\n",
      "  (0, 95162)\t1\n",
      "  (0, 87620)\t1\n",
      "  (0, 37037)\t1\n",
      "  (0, 89860)\t3\n",
      "  (0, 32567)\t1\n",
      "  (0, 26048)\t4\n",
      "  (0, 67798)\t1\n",
      "  (0, 99721)\t1\n",
      "  (0, 111322)\t1\n",
      "  (0, 125627)\t1\n",
      "  (0, 70553)\t2\n",
      "  (0, 105620)\t3\n",
      "  (0, 73730)\t3\n",
      "  (0, 85921)\t3\n",
      "  (0, 62752)\t2\n",
      "  (0, 71165)\t1\n",
      "  (0, 44752)\t1\n",
      "  (0, 56979)\t2\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of words in a document is also trivial to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts[0,:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, we can count how many times a certain word occurs in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also learn more about the vocabulary, e.g., how many times a word occurs in the corpus. First, we need to find the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107529"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'sin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the index, we can count how many times the word \"sin\" occurs in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sin_index = count_vect.vocabulary_.get(u'sin')\n",
    "X_train_counts[:,sin_index].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. `CountVectorizer` lets you also define if you want to count bigrams, or other n-grams. Moreover, you can not only count words, but als characters. We suggest you try these out for yourself. In the following, we will continue with unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have numbers now instead of strings, we could start training models now. However, raw counts will not be very informative, since we also have to take the length of a dodument into account. Dividing each row by the total number of words will give us the term frequency for each document. That will be much better! Now we still might have higher values for words which occur often in many documents. typically, these words are less informative, so we need to downscale those weights. This will modify or counts so that we are left with what is called the \"term frequency-inverse document frequency\" measure, or tf-idf. The tf-idf measure is given by\n",
    "\\begin{equation}\n",
    "f_{t,d}\\cdot log \\frac{N}{n_t}\n",
    "\\end{equation}\n",
    "In sklearn, there is the `TfidfTransformer` which does exactly that for us :-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_tranformer = TfidfTransformer(smooth_idf=True).fit(X_train_counts)\n",
    "X_train_tfidf = tfidf_tranformer.transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x130107 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 127 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56979)\t0.02565304152132985\n",
      "  (0, 44752)\t0.09478087712757985\n",
      "  (0, 71165)\t0.09478087712757985\n",
      "  (0, 62752)\t0.19562584539401784\n",
      "  (0, 85921)\t0.2753532341667019\n",
      "  (0, 73730)\t0.269421583481477\n",
      "  (0, 105620)\t0.18822150321045295\n",
      "  (0, 70553)\t0.12854417363144774\n",
      "  (0, 125627)\t0.10758852789712267\n",
      "  (0, 111322)\t0.012826520760664924\n",
      "  (0, 99721)\t0.017602991837857778\n",
      "  (0, 67798)\t0.054830315362812916\n",
      "  (0, 26048)\t0.3693212126672368\n",
      "  (0, 32567)\t0.08024716614572393\n",
      "  (0, 89860)\t0.0574060439262081\n",
      "  (0, 37037)\t0.08731684834379078\n",
      "  (0, 87620)\t0.023884357046512178\n",
      "  (0, 95162)\t0.023080567513977284\n",
      "  (0, 64095)\t0.023716339089827933\n",
      "  (0, 90379)\t0.013343514042299711\n",
      "  (0, 103113)\t0.07900491984199746\n",
      "  (0, 67602)\t0.050097559090540596\n",
      "  (0, 89362)\t0.0727716664155614\n",
      "  (0, 113812)\t0.04756705206656423\n",
      "  (0, 110775)\t0.08186707523553768\n",
      "  :\t:\n",
      "  (0, 86864)\t0.033785596330043605\n",
      "  (0, 83858)\t0.0700159651439916\n",
      "  (0, 114806)\t0.04450906812477559\n",
      "  (0, 41525)\t0.08162275316729287\n",
      "  (0, 101005)\t0.06729162480662827\n",
      "  (0, 93699)\t0.07940578124007644\n",
      "  (0, 95130)\t0.04523984671699543\n",
      "  (0, 104494)\t0.03747555640481748\n",
      "  (0, 59666)\t0.05393339505986993\n",
      "  (0, 106209)\t0.06531440791155332\n",
      "  (0, 95064)\t0.0745489697635122\n",
      "  (0, 76551)\t0.10301362932388351\n",
      "  (0, 29620)\t0.02293265915656202\n",
      "  (0, 123759)\t0.03379721982042733\n",
      "  (0, 39203)\t0.06819354083339708\n",
      "  (0, 116834)\t0.09869786119547418\n",
      "  (0, 78586)\t0.05595727086693062\n",
      "  (0, 61538)\t0.08589021042593134\n",
      "  (0, 46170)\t0.09178441138890063\n",
      "  (0, 114731)\t0.019346558116940304\n",
      "  (0, 29573)\t0.02261995182951863\n",
      "  (0, 95956)\t0.07650297123005821\n",
      "  (0, 2337)\t0.05245684843794907\n",
      "  (0, 100156)\t0.10067507809054912\n",
      "  (0, 51052)\t0.10179113132392065\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we apply the transformation to the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_tranformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should suffice as features to train a classifer (for the moment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorise labels\n",
    "Next, we deal with the labels. Every document has exactly one label attached. We have 20 labels in total. This means we can basically assign a number to each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
       "       'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
       "       'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles',\n",
       "       'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt',\n",
       "       'sci.electronics', 'sci.med', 'sci.space',\n",
       "       'soc.religion.christian', 'talk.politics.guns',\n",
       "       'talk.politics.mideast', 'talk.politics.misc',\n",
       "       'talk.religion.misc'], dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, let's train models\n",
    "Now it's time to train models. Let's stich to the Multinomial Naive Bayes classifier for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well we do on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  3, 17, ...,  9,  1,  3])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  2, 17, ...,  9,  1,  6])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the accuracy is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7738980350504514\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for index, prediction in enumerate(nb_clf.predict(X_test_tfidf)):\n",
    "    if prediction == y_test[index]:\n",
    "        correct +=1\n",
    "\n",
    "print('Accuracy: ', correct/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 80 percent, that is not too bad. What about a Support Vector Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8531598513011153\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for index, prediction in enumerate(svc.predict(X_test_tfidf)):\n",
    "    if prediction == y_test[index]:\n",
    "        correct +=1\n",
    "\n",
    "print('Accuracy: ', correct/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An increase of 8%, that's good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in order to determine the performance of our models we need cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(nb_clf, X_train_tfidf, y_train, scoring='accuracy', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85526316, 0.8427065 , 0.85413005, 0.84859155, 0.8524735 ,\n",
       "       0.83613818, 0.84663121, 0.84280639, 0.86032028, 0.85752449])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(svc, X_train_tfidf, y_train, scoring='accuracy', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94473684, 0.92003515, 0.93936731, 0.92869718, 0.92579505,\n",
       "       0.93356953, 0.92287234, 0.92007105, 0.92882562, 0.92430988])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate precision, recall, and f1 relatively easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=50)\n",
    "sgd_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_train_predictions = cross_val_predict(sgd_clf, X_train_tfidf, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9170054799363621\n",
      "0.9170054799363621\n",
      "0.9170054799363621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[440,   0,   0,   0,   0,   0,   0,   1,   1,   0,   0,   0,   0,\n",
       "          2,   2,  13,   1,   2,   1,  17],\n",
       "       [  0, 502,  13,  20,   6,  19,   7,   1,   1,   1,   1,   0,   5,\n",
       "          3,   2,   1,   2,   0,   0,   0],\n",
       "       [  0,  15, 528,  18,   5,  15,   3,   1,   0,   1,   0,   0,   2,\n",
       "          0,   1,   1,   0,   0,   1,   0],\n",
       "       [  0,  15,  32, 467,  18,   8,  17,   2,   0,   1,   3,   0,  21,\n",
       "          1,   1,   1,   1,   0,   1,   1],\n",
       "       [  1,   7,   5,  26, 508,   4,   9,   1,   3,   3,   1,   1,   6,\n",
       "          1,   1,   0,   0,   0,   1,   0],\n",
       "       [  0,  20,  13,   9,   1, 530,   6,   3,   2,   1,   1,   2,   3,\n",
       "          1,   1,   0,   0,   0,   0,   0],\n",
       "       [  1,   3,   2,  14,   6,   1, 516,  12,   4,   0,   6,   0,  15,\n",
       "          0,   2,   1,   0,   1,   1,   0],\n",
       "       [  0,   3,   4,   3,   1,   3,  10, 536,  12,   4,   1,   0,   9,\n",
       "          0,   3,   0,   4,   1,   0,   0],\n",
       "       [  1,   1,   0,   0,   2,   1,   8,   5, 578,   0,   0,   0,   1,\n",
       "          1,   0,   0,   0,   0,   0,   0],\n",
       "       [  1,   2,   1,   0,   1,   1,   2,   5,   0, 573,   8,   1,   1,\n",
       "          1,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   1,   1,   0,   1,   1,   3,   1,   1,   1, 585,   0,   0,\n",
       "          0,   0,   0,   0,   2,   2,   1],\n",
       "       [  0,   8,   2,   1,   1,   1,   0,   1,   1,   0,   1, 570,   2,\n",
       "          1,   0,   1,   2,   1,   2,   0],\n",
       "       [  0,  11,   5,  15,   8,   1,  11,   6,   1,   3,   2,   2, 519,\n",
       "          2,   3,   1,   0,   0,   1,   0],\n",
       "       [  0,   5,   2,   0,   3,   4,   3,   1,   1,   0,   0,   2,   3,\n",
       "        567,   2,   1,   0,   0,   0,   0],\n",
       "       [  0,   6,   0,   1,   1,   0,   2,   0,   0,   0,   0,   0,   0,\n",
       "          4, 577,   0,   1,   1,   0,   0],\n",
       "       [  7,   2,   0,   2,   2,   1,   4,   2,   0,   0,   1,   0,   1,\n",
       "          2,   0, 568,   0,   2,   0,   5],\n",
       "       [  0,   0,   1,   0,   0,   1,   2,   2,   0,   2,   0,   2,   1,\n",
       "          0,   1,   0, 529,   0,   2,   3],\n",
       "       [  0,   2,   0,   0,   0,   0,   0,   1,   0,   1,   0,   0,   0,\n",
       "          1,   1,   4,   0, 553,   1,   0],\n",
       "       [  2,   1,   0,   0,   0,   1,   3,   2,   1,   2,   2,   1,   1,\n",
       "          0,   1,   1,   8,   4, 432,   3],\n",
       "       [ 18,   0,   0,   0,   1,   2,   2,   2,   1,   0,   2,   0,   1,\n",
       "          3,   0,  28,  13,   2,   5, 297]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(precision_score(y_train, y_train_predictions, average='micro'))\n",
    "print(recall_score(y_train, y_train_predictions, average='micro'))\n",
    "print(f1_score(y_train, y_train_predictions, average='micro'))\n",
    "conf_mx = confusion_matrix(y_train, y_train_predictions)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f00b62baba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f00b6955e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
       "       'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
       "       'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles',\n",
       "       'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt',\n",
       "       'sci.electronics', 'sci.med', 'sci.space',\n",
       "       'soc.religion.christian', 'talk.politics.guns',\n",
       "       'talk.politics.mideast', 'talk.politics.misc',\n",
       "       'talk.religion.misc'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcuts in sklearn - pipelines\n",
    "Sklearn allows us to build convenient `Pipelines`, which facilitate the management of our data and the training of our models enourmously. Consider for ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('nb_clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could even replace the two first lines of the pipeline by using `TfidfVectorizer`, which first fits and transforms the input the same way as the `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ar_tf=False, use_idf=True)), ('nb_clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(text_clf, X_train, y_train, scoring='accuracy', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85614035, 0.84446397, 0.85588752, 0.85123239, 0.85424028,\n",
       "       0.83879539, 0.84840426, 0.84547069, 0.86120996, 0.85841496])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection - find your best model\n",
    "For every model you would like to train, there is a plethora of parameters you could set. How to find the best model? Again, sklearn has a solution: `GridSearchCV`. With grid search cross validation, you can set your hyperparameter space and train different models with all the parameter combinations. Keep in mind that depending on how many folds you train, the whole training procedure takes significantly longer. But let's set up grid search cross validation. We set up a new pipeline for a SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  40 out of  40 | elapsed: 49.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid=True, n_jobs=4,\n",
       "       param_grid={'vect__ngram_range': [(1, 1), (1, 2)], 'svc__loss': ['hinge', 'squared_hinge'], 'svc__multi_class': ['ovr', 'crammer_singer']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "text_svc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svc', LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "             'svc__loss': ['hinge', 'squared_hinge'],\n",
    "             'svc__multi_class': ['ovr', 'crammer_singer']}\n",
    "\n",
    "gs_svc = GridSearchCV(text_svc, param_grid, cv=5, n_jobs=4, verbose=1)\n",
    "gs_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "text_svc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svc', LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "             'svc__loss': ['hinge', 'squared_hinge'],\n",
    "             'svc__multi_class': ['ovr', 'crammer_singer']}\n",
    "\n",
    "gs_svc = GridSearchCV(text_svc, param_grid, cv=10, n_jobs=3, verbose=1, return_train_score=True)\n",
    "gs_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phillip/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/phillip/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/phillip/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/phillip/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/phillip/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/phillip/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/phillip/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_svc__loss</th>\n",
       "      <th>param_svc__multi_class</th>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>686.134789</td>\n",
       "      <td>3.743508</td>\n",
       "      <td>0.930705</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'svc__loss': 'hinge', 'svc__multi_class': 'cr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924912</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.926991</td>\n",
       "      <td>0.999116</td>\n",
       "      <td>0.934368</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>286.866087</td>\n",
       "      <td>0.126190</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>662.552173</td>\n",
       "      <td>2.885766</td>\n",
       "      <td>0.930705</td>\n",
       "      <td>0.999359</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'svc__loss': 'squared_hinge', 'svc__multi_cla...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924912</td>\n",
       "      <td>0.999448</td>\n",
       "      <td>0.926991</td>\n",
       "      <td>0.999006</td>\n",
       "      <td>0.934368</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>208.876589</td>\n",
       "      <td>0.894399</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57.669994</td>\n",
       "      <td>3.633046</td>\n",
       "      <td>0.930263</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>ovr</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'svc__loss': 'squared_hinge', 'svc__multi_cla...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.936536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924028</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.923451</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.932151</td>\n",
       "      <td>0.999890</td>\n",
       "      <td>1.424082</td>\n",
       "      <td>0.317880</td>\n",
       "      <td>0.005513</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>339.978553</td>\n",
       "      <td>3.932459</td>\n",
       "      <td>0.928938</td>\n",
       "      <td>0.998387</td>\n",
       "      <td>hinge</td>\n",
       "      <td>ovr</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'svc__loss': 'hinge', 'svc__multi_class': 'ov...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.934332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926237</td>\n",
       "      <td>0.998232</td>\n",
       "      <td>0.921239</td>\n",
       "      <td>0.998012</td>\n",
       "      <td>0.930377</td>\n",
       "      <td>0.999007</td>\n",
       "      <td>47.562352</td>\n",
       "      <td>0.643115</td>\n",
       "      <td>0.004697</td>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.755919</td>\n",
       "      <td>1.589989</td>\n",
       "      <td>0.924607</td>\n",
       "      <td>0.999138</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>ovr</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'svc__loss': 'squared_hinge', 'svc__multi_cla...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.929925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924912</td>\n",
       "      <td>0.999227</td>\n",
       "      <td>0.915044</td>\n",
       "      <td>0.999006</td>\n",
       "      <td>0.925055</td>\n",
       "      <td>0.999448</td>\n",
       "      <td>0.882834</td>\n",
       "      <td>0.366745</td>\n",
       "      <td>0.005138</td>\n",
       "      <td>0.000190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.151473</td>\n",
       "      <td>1.638614</td>\n",
       "      <td>0.924342</td>\n",
       "      <td>0.995139</td>\n",
       "      <td>hinge</td>\n",
       "      <td>ovr</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'svc__loss': 'hinge', 'svc__multi_class': 'ov...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.930366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924028</td>\n",
       "      <td>0.995359</td>\n",
       "      <td>0.913717</td>\n",
       "      <td>0.994478</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.995474</td>\n",
       "      <td>9.684970</td>\n",
       "      <td>0.225647</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.000363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184.052793</td>\n",
       "      <td>1.491734</td>\n",
       "      <td>0.924076</td>\n",
       "      <td>0.998564</td>\n",
       "      <td>hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'svc__loss': 'hinge', 'svc__multi_class': 'cr...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.929044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922261</td>\n",
       "      <td>0.998674</td>\n",
       "      <td>0.919469</td>\n",
       "      <td>0.998343</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.999117</td>\n",
       "      <td>52.200238</td>\n",
       "      <td>0.116839</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>185.887219</td>\n",
       "      <td>1.562905</td>\n",
       "      <td>0.924076</td>\n",
       "      <td>0.998564</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'svc__loss': 'squared_hinge', 'svc__multi_cla...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.929044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922261</td>\n",
       "      <td>0.998674</td>\n",
       "      <td>0.919469</td>\n",
       "      <td>0.998343</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.999117</td>\n",
       "      <td>41.081316</td>\n",
       "      <td>0.157256</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "3     686.134789         3.743508         0.930705          0.999403   \n",
       "7     662.552173         2.885766         0.930705          0.999359   \n",
       "5      57.669994         3.633046         0.930263          0.999669   \n",
       "1     339.978553         3.932459         0.928938          0.998387   \n",
       "4      12.755919         1.589989         0.924607          0.999138   \n",
       "0      55.151473         1.638614         0.924342          0.995139   \n",
       "2     184.052793         1.491734         0.924076          0.998564   \n",
       "6     185.887219         1.562905         0.924076          0.998564   \n",
       "\n",
       "  param_svc__loss param_svc__multi_class param_vect__ngram_range  \\\n",
       "3           hinge         crammer_singer                  (1, 2)   \n",
       "7   squared_hinge         crammer_singer                  (1, 2)   \n",
       "5   squared_hinge                    ovr                  (1, 2)   \n",
       "1           hinge                    ovr                  (1, 2)   \n",
       "4   squared_hinge                    ovr                  (1, 1)   \n",
       "0           hinge                    ovr                  (1, 1)   \n",
       "2           hinge         crammer_singer                  (1, 1)   \n",
       "6   squared_hinge         crammer_singer                  (1, 1)   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "3  {'svc__loss': 'hinge', 'svc__multi_class': 'cr...                1   \n",
       "7  {'svc__loss': 'squared_hinge', 'svc__multi_cla...                1   \n",
       "5  {'svc__loss': 'squared_hinge', 'svc__multi_cla...                3   \n",
       "1  {'svc__loss': 'hinge', 'svc__multi_class': 'ov...                4   \n",
       "4  {'svc__loss': 'squared_hinge', 'svc__multi_cla...                5   \n",
       "0  {'svc__loss': 'hinge', 'svc__multi_class': 'ov...                6   \n",
       "2  {'svc__loss': 'hinge', 'svc__multi_class': 'cr...                7   \n",
       "6  {'svc__loss': 'squared_hinge', 'svc__multi_cla...                7   \n",
       "\n",
       "   split0_test_score       ...         split2_test_score  split2_train_score  \\\n",
       "3           0.937417       ...                  0.924912            0.999558   \n",
       "7           0.937417       ...                  0.924912            0.999448   \n",
       "5           0.936536       ...                  0.924028            0.999669   \n",
       "1           0.934332       ...                  0.926237            0.998232   \n",
       "4           0.929925       ...                  0.924912            0.999227   \n",
       "0           0.930366       ...                  0.924028            0.995359   \n",
       "2           0.929044       ...                  0.922261            0.998674   \n",
       "6           0.929044       ...                  0.922261            0.998674   \n",
       "\n",
       "   split3_test_score  split3_train_score  split4_test_score  \\\n",
       "3           0.926991            0.999116           0.934368   \n",
       "7           0.926991            0.999006           0.934368   \n",
       "5           0.923451            0.999558           0.932151   \n",
       "1           0.921239            0.998012           0.930377   \n",
       "4           0.915044            0.999006           0.925055   \n",
       "0           0.913717            0.994478           0.927273   \n",
       "2           0.919469            0.998343           0.926829   \n",
       "6           0.919469            0.998343           0.926829   \n",
       "\n",
       "   split4_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "3            0.999669    286.866087        0.126190        0.004619   \n",
       "7            0.999669    208.876589        0.894399        0.004619   \n",
       "5            0.999890      1.424082        0.317880        0.005513   \n",
       "1            0.999007     47.562352        0.643115        0.004697   \n",
       "4            0.999448      0.882834        0.366745        0.005138   \n",
       "0            0.995474      9.684970        0.225647        0.005686   \n",
       "2            0.999117     52.200238        0.116839        0.003420   \n",
       "6            0.999117     41.081316        0.157256        0.003420   \n",
       "\n",
       "   std_train_score  \n",
       "3         0.000193  \n",
       "7         0.000214  \n",
       "5         0.000121  \n",
       "1         0.000332  \n",
       "4         0.000190  \n",
       "0         0.000363  \n",
       "2         0.000313  \n",
       "6         0.000313  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_df = pd.DataFrame.from_dict(gs_svc.cv_results_)\n",
    "svc_df.sort_values(by=[\"rank_test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  2, 17, ...,  9,  1,  2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  2, 17, ...,  9,  1,  6])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip...0,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=None,\n",
       "     tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svc', LinearSVC(loss='hinge', multi_class='crammer_singer'))\n",
    "])\n",
    "\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  2, 17, ...,  9,  1,  2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8600637280934679\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for index, prediction in enumerate(best_model.predict(X_test)):\n",
    "    if prediction == y_test[index]:\n",
    "        correct +=1\n",
    "\n",
    "print('Accuracy: ', correct/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
